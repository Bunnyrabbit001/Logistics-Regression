{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Theoritical Quesions**"
      ],
      "metadata": {
        "id": "8zj9u8UMRUn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Logistic Regression, and how does it differ from Linear Regression.**\n",
        "\n",
        "**Logistic Regression** and **Linear Regression** are both statistical models used in machine learning, but they serve different purposes and are used for different types of problems.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Logistic Regression:**\n",
        "\n",
        "* **Purpose**: Used for **classification problems**, especially binary classification (e.g., yes/no, 0/1, true/false).\n",
        "* **Output**: Predicts the **probability** that a given input belongs to a particular class.\n",
        "* **Function Used**: Applies the **sigmoid (logistic) function** to the output of a linear equation to map predictions between **0 and 1**.\n",
        "\n",
        "  $$\n",
        "  \\text{Sigmoid function: } \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "\n",
        "  where $z = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n$\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Linear Regression:**\n",
        "\n",
        "* **Purpose**: Used for **regression problems**, i.e., predicting a **continuous numerical value** (e.g., house price, salary).\n",
        "* **Output**: Directly predicts a continuous value using a **linear equation**.\n",
        "* **Function Used**: A simple linear function without any transformation.\n",
        "\n",
        "  $$\n",
        "  y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences:**\n",
        "\n",
        "| Feature               | Logistic Regression         | Linear Regression            |\n",
        "| --------------------- | --------------------------- | ---------------------------- |\n",
        "| **Goal**              | Classification              | Regression                   |\n",
        "| **Output**            | Probability (0 to 1)        | Continuous value             |\n",
        "| **Function used**     | Sigmoid (logistic) function | Linear function              |\n",
        "| **Target Variable**   | Categorical (e.g., 0 or 1)  | Continuous (e.g., 0.5, 42.3) |\n",
        "| **Loss Function**     | Log Loss (Cross-Entropy)    | Mean Squared Error (MSE)     |\n",
        "| **Prediction Format** | Probability or class label  | Real number                  |\n",
        "\n",
        "---\n",
        "\n",
        "In short:\n",
        "\n",
        "* Use **Linear Regression** when predicting quantities.\n",
        "* Use **Logistic Regression** when classifying outcomes.\n"
      ],
      "metadata": {
        "id": "6YwQfZhoRdK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the mathematical equation of Logistic Regression.**\n",
        "\n",
        "The **mathematical equation of Logistic Regression** is based on the **sigmoid (logistic) function**, which transforms the linear combination of inputs into a probability between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **Logistic Regression Equation**\n",
        "\n",
        "#### Step 1: Linear combination of features\n",
        "\n",
        "$$\n",
        "z = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n = \\mathbf{w}^T\\mathbf{x}\n",
        "$$\n",
        "\n",
        "* $b_0$: Intercept (bias term)\n",
        "* $b_1, b_2, ..., b_n$: Coefficients (weights)\n",
        "* $x_1, x_2, ..., x_n$: Feature values\n",
        "* $\\mathbf{w}$: Weight vector\n",
        "* $\\mathbf{x}$: Input feature vector\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: Apply the sigmoid function\n",
        "\n",
        "$$\n",
        "P(Y=1 \\mid X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "This gives the **probability** that the output $Y$ is 1 (positive class), given input features $X$.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Logistic Regression Model:\n",
        "\n",
        "$$\n",
        "P(Y=1 \\mid X) = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n)}}\n",
        "$$\n",
        "\n",
        "To classify the result into a binary label (0 or 1), we usually use a **threshold**:\n",
        "\n",
        "* If $P(Y=1 \\mid X) \\geq 0.5$, predict class 1\n",
        "* If $P(Y=1 \\mid X) < 0.5$, predict class 0\n"
      ],
      "metadata": {
        "id": "JETZwSfHRdHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Why do we use the Sigmoid function in Logistic Regression.**\n",
        "\n",
        "We use the **sigmoid function** in **logistic regression** because it **converts any real-valued number into a value between 0 and 1**, which can be interpreted as a **probability**. This is essential because logistic regression is used for **classification**, where the output should reflect the **probability of belonging to a particular class**, especially in **binary classification**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Reasons for Using the Sigmoid Function:**\n",
        "\n",
        "#### 1. **Probability Output**\n",
        "\n",
        "* The sigmoid function maps input $z$ (which can range from $-\\infty$ to $+\\infty$) to a value between 0 and 1:\n",
        "\n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "* This output can be directly interpreted as:\n",
        "\n",
        "  $$\n",
        "  P(Y = 1 \\mid X)\n",
        "  $$\n",
        "\n",
        "#### 2. **Decision Boundary**\n",
        "\n",
        "* By setting a **threshold** (commonly 0.5), we can classify:\n",
        "\n",
        "  * If $\\sigma(z) \\geq 0.5$, predict class 1\n",
        "  * If $\\sigma(z) < 0.5$, predict class 0\n",
        "\n",
        "#### 3. **Differentiability**\n",
        "\n",
        "* The sigmoid function is **smooth and differentiable**, which makes it suitable for **gradient-based optimization** (like Gradient Descent).\n",
        "\n",
        "#### 4. **Monotonic Function**\n",
        "\n",
        "* As the input increases, the sigmoid output increases — this ensures consistent interpretation of the relationship between features and the predicted probability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Not Use Linear Output Instead?**\n",
        "\n",
        "* Linear regression can output values less than 0 or greater than 1, which makes no sense when modeling probabilities.\n",
        "* It also doesn’t handle the **non-linear** nature of classification boundaries well.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "The sigmoid function is used in logistic regression to:\n",
        "\n",
        "* Map outputs to probabilities (0 to 1),\n",
        "* Enable binary classification,\n",
        "* Allow smooth optimization,\n",
        "* Maintain interpretability of outputs as probabilities.\n"
      ],
      "metadata": {
        "id": "zYASpSrHRdFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the cost function of Logistic Regression.**\n",
        "\n",
        "The **cost function of logistic regression** is based on **log loss**, also known as **binary cross-entropy loss**. It measures how well the predicted probabilities match the actual binary outcomes (0 or 1).\n",
        "\n",
        "---\n",
        "\n",
        "**Why Not Use MSE (Mean Squared Error)?**\n",
        "\n",
        "* MSE works well for regression but performs poorly in classification tasks.\n",
        "* Logistic regression uses a **non-linear sigmoid** activation, and MSE would lead to a **non-convex cost function**, making optimization difficult.\n",
        "* **Log loss** provides a **convex cost function**, ensuring better and faster convergence with methods like gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "**Mathematical Form of the Cost Function:**\n",
        "\n",
        "For a single training example $(x, y)$:\n",
        "\n",
        "$$\n",
        "\\text{Cost}(h_\\theta(x), y) = -y \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y \\in \\{0, 1\\}$: actual label\n",
        "* $h_\\theta(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$: predicted probability\n",
        "* $\\log$: natural logarithm\n",
        "\n",
        "---\n",
        "\n",
        "**Full Cost Function (for m training examples):**\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Intuition Behind It:**\n",
        "\n",
        "* If the true label is 1 and the predicted probability is close to 1 → **low cost**\n",
        "* If the true label is 1 but the predicted probability is close to 0 → **high cost**\n",
        "* It heavily penalizes **confident wrong predictions**\n"
      ],
      "metadata": {
        "id": "OrzSGKl1RdCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is Regularization in Logistic Regression? Why is it needed.**\n",
        "\n",
        "### **What is Regularization in Logistic Regression?**\n",
        "\n",
        "**Regularization** is a technique used in logistic regression (and other models) to **prevent overfitting** by **penalizing large coefficients** (weights) in the model.\n",
        "\n",
        "It adds a **penalty term** to the cost function that discourages complex models (i.e., models with large weights), which helps the model generalize better to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Regularization in Logistic Regression:**\n",
        "\n",
        "1. **L1 Regularization (Lasso):**\n",
        "\n",
        "   * Adds the **sum of the absolute values** of the coefficients.\n",
        "\n",
        "   $$\n",
        "   \\text{Penalty} = \\lambda \\sum_{j=1}^{n} | \\theta_j |\n",
        "   $$\n",
        "\n",
        "   * Can shrink some weights to **exactly zero** → useful for **feature selection**.\n",
        "\n",
        "2. **L2 Regularization (Ridge):**\n",
        "\n",
        "   * Adds the **sum of the squares** of the coefficients.\n",
        "\n",
        "   $$\n",
        "   \\text{Penalty} = \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "   $$\n",
        "\n",
        "   * Tends to shrink weights smoothly but doesn't force them to zero.\n",
        "\n",
        "3. **Elastic Net** (a combination of L1 and L2) is sometimes used too.\n",
        "\n",
        "---\n",
        "\n",
        "### **Modified Cost Function (with L2 Regularization):**\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "* $\\lambda$: Regularization parameter (controls the strength of the penalty)\n",
        "* Larger $\\lambda$: more regularization → simpler model\n",
        "* Smaller $\\lambda$: less regularization → model can fit more complex patterns\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Regularization Needed?**\n",
        "\n",
        "1. **Prevents Overfitting**:\n",
        "\n",
        "   * Without regularization, the model might learn noise from the training data and perform poorly on test data.\n",
        "\n",
        "2. **Simplifies the Model**:\n",
        "\n",
        "   * Regularization forces the model to focus on the **most important features** by keeping weights small or zero.\n",
        "\n",
        "3. **Improves Generalization**:\n",
        "\n",
        "   * A regularized model is more likely to perform well on unseen data.\n"
      ],
      "metadata": {
        "id": "pGiAeEe3Rc_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "Here’s a clear comparison of **Lasso**, **Ridge**, and **Elastic Net** regression techniques — all of which are used to prevent **overfitting** by applying **regularization** in models like linear or logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "**Penalty term**: Sum of the **squares** of the coefficients.\n",
        "\n",
        "  $$\n",
        "  \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "  $$\n",
        "**Effect**: Shrinks the weights (coefficients) towards zero, but **never exactly zero**.\n",
        "**Use case**: When **all features are useful** but you want to reduce model complexity.\n",
        "\n",
        "Keeps all features\n",
        "No feature elimination\n",
        "\n",
        "---\n",
        "\n",
        "**2. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "**Penalty term**: Sum of the **absolute values** of the coefficients.\n",
        "\n",
        "  $$\n",
        "  \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "  $$\n",
        "**Effect**: Shrinks some weights to **exactly zero**, effectively **removing unimportant features**.\n",
        "**Use case**: Useful for **feature selection** when you suspect many features are irrelevant.\n",
        "\n",
        "Feature elimination\n",
        "Can be unstable when features are highly correlated\n",
        "\n",
        "---\n",
        "\n",
        "**3. Elastic Net Regression (L1 + L2 Regularization)**\n",
        "\n",
        "* **Penalty term**: Combination of both L1 and L2.\n",
        "\n",
        "  $$\n",
        "  \\text{Loss} = \\text{Original Loss} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2\n",
        "  $$\n",
        "* **Effect**: Combines the benefits of **Ridge** and **Lasso**:\n",
        "\n",
        "  * Performs **feature selection** like Lasso\n",
        "  * Handles **multicollinearity** like Ridge\n",
        "* **Use case**: When you have **many features**, some of which are correlated and some possibly irrelevant."
      ],
      "metadata": {
        "id": "gnuod44ERc8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Q6. When should we use Elastic Net instead of Lasso or Ridge.**\n",
        "\n",
        "Here’s a clean explanation without emojis:\n",
        "\n",
        "---\n",
        "**When to Use Elastic Net Instead of Lasso or Ridge**\n",
        "\n",
        "You should use **Elastic Net** when your dataset has **many features**, especially if:\n",
        "\n",
        "1. **There are many irrelevant features** (some don't contribute to the target variable).\n",
        "2. **Some features are highly correlated** with each other.\n",
        "\n",
        "**Use Elastic Net When:**\n",
        "\n",
        "1. **High-Dimensional Data:**\n",
        "\n",
        "   * Situations where the number of features (variables) is larger than the number of samples.\n",
        "   * Example: Text data, gene expression datasets.\n",
        "   * Lasso alone may fail to perform well here, while Elastic Net is more stable.\n",
        "\n",
        "2. **Correlated Features:**\n",
        "\n",
        "   * Lasso may pick only one feature from a group of correlated features and ignore the rest.\n",
        "   * Elastic Net tends to **retain all relevant correlated features**, which improves stability.\n",
        "\n",
        "3. **You Need a Balanced Approach:**\n",
        "\n",
        "   * Elastic Net combines **feature selection** (Lasso) and **coefficient shrinkage** (Ridge).\n",
        "   * This makes it more flexible and general-purpose.\n",
        "\n",
        "4. **Lasso is Too Aggressive:**\n",
        "\n",
        "   * If Lasso removes too many features or important ones, Elastic Net provides a middle ground.\n",
        "\n",
        "---\n",
        "\n",
        "**Avoid Elastic Net When:**\n",
        "\n",
        "* The number of features is small and there's no strong correlation between them — in such cases, **Lasso** or **Ridge** alone may be simpler and sufficient.\n",
        "* You want a simpler model with fewer tuning parameters — Elastic Net requires tuning both $\\lambda$ and the mixing parameter $\\alpha$, which adds complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "pWWs8QhBRc6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.  What is the impact of the regularization parameter (λ) in Logistic Regression.**\n",
        "\n",
        "The **regularization parameter $\\lambda$** in **logistic regression** controls the **strength of regularization** applied to the model. It directly affects how much the model **penalizes large coefficients** (weights), which helps manage the trade-off between **underfitting** and **overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Impact of $\\lambda$:**\n",
        "\n",
        "#### 1. **When $\\lambda = 0$**:\n",
        "\n",
        "* **No regularization** is applied.\n",
        "* The model focuses entirely on minimizing the **log loss**.\n",
        "* Risk of **overfitting**, especially with many features or noisy data.\n",
        "\n",
        "#### 2. **When $\\lambda$ is small (close to 0)**:\n",
        "\n",
        "* **Weak regularization**.\n",
        "* Coefficients are only slightly penalized.\n",
        "* The model can still fit the training data well but might overfit on complex datasets.\n",
        "\n",
        "#### 3. **When $\\lambda$ is large**:\n",
        "\n",
        "* **Strong regularization**.\n",
        "* Forces coefficients to shrink closer to zero.\n",
        "* Reduces model complexity, improves generalization, but can lead to **underfitting** if too strong.\n",
        "\n",
        "---\n",
        "\n",
        "### **Effect on Model Behavior:**\n",
        "\n",
        "| $\\lambda$ Value | Regularization Strength | Coefficient Magnitude | Risk                |\n",
        "| --------------- | ----------------------- | --------------------- | ------------------- |\n",
        "| 0               | None                    | Can be large          | Overfitting         |\n",
        "| Small           | Low                     | Moderately reduced    | Slight overfitting  |\n",
        "| Moderate        | Balanced                | Controlled            | Good generalization |\n",
        "| Large           | High                    | Near zero             | Underfitting        |\n",
        "\n",
        "---\n",
        "\n",
        "### **In Practice:**\n",
        "\n",
        "* Use **cross-validation** to find the optimal $\\lambda$.\n",
        "* Common libraries like scikit-learn use **inverse regularization strength** $C = \\frac{1}{\\lambda}$, so:\n",
        "\n",
        "  * Larger $C$ = less regularization\n",
        "  * Smaller $C$ = more regularization\n"
      ],
      "metadata": {
        "id": "ozf1C4O2Rc3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What are the key assumptions of Logistic Regression.**\n",
        "\n",
        "Here are the **key assumptions of Logistic Regression**, which are important for the model to perform reliably and give interpretable results:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Binary or Categorical Outcome Variable**\n",
        "\n",
        "* The **dependent variable** should be **binary** (0 or 1) for binary logistic regression.\n",
        "* For multiple categories, use **multinomial logistic regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Linearity of Logit**\n",
        "\n",
        "* Logistic regression **assumes a linear relationship** between the **log-odds** of the dependent variable and the independent variables.\n",
        "\n",
        "  $$\n",
        "  \\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
        "  $$\n",
        "* Note: This is **not** the same as assuming linearity between predictors and the output itself (as in linear regression).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **No (or minimal) Multicollinearity**\n",
        "\n",
        "* Independent variables should **not be highly correlated** with each other.\n",
        "* High multicollinearity can inflate standard errors and make coefficient estimates unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Independence of Observations**\n",
        "\n",
        "* Observations should be **independent** of each other.\n",
        "* Logistic regression is **not suitable** for time-series or grouped data unless additional methods (e.g., mixed models) are applied.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Large Sample Size**\n",
        "\n",
        "* Logistic regression performs better with a **larger sample size**, especially if the event (class 1) is rare.\n",
        "* Helps ensure the stability and accuracy of the estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **No Extreme Outliers**\n",
        "\n",
        "* While logistic regression is more robust than linear regression, **extreme outliers** in the independent variables can still affect the model.\n",
        "* Detecting and handling outliers is important for reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### Optional:\n",
        "\n",
        "* **Independence of errors** and **homoscedasticity** (constant variance of errors) are **not required** assumptions in logistic regression, unlike in linear regression.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CxAjGxVYRc00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What are some alternatives to Logistic Regression for classification tasks.**\n",
        "\n",
        "Here are some common **alternatives to Logistic Regression** for classification tasks, each with its own strengths and typical use cases:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Decision Trees**\n",
        "\n",
        "* Non-linear model that splits data based on feature thresholds.\n",
        "* Easy to interpret.\n",
        "* Can handle both categorical and numerical data.\n",
        "* Prone to overfitting unless pruned.\n",
        "\n",
        "### 2. **Random Forest**\n",
        "\n",
        "* An ensemble of decision trees using bagging.\n",
        "* Reduces overfitting compared to a single tree.\n",
        "* Works well with high-dimensional data.\n",
        "\n",
        "### 3. **Support Vector Machines (SVM)**\n",
        "\n",
        "* Finds the optimal hyperplane that separates classes with maximum margin.\n",
        "* Effective in high-dimensional spaces.\n",
        "* Can use different kernels to model non-linear decision boundaries.\n",
        "\n",
        "### 4. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "* Classifies based on the majority label among the k closest data points.\n",
        "* Simple and non-parametric.\n",
        "* Computationally expensive for large datasets.\n",
        "\n",
        "### 5. **Naive Bayes**\n",
        "\n",
        "* Probabilistic classifier based on Bayes’ theorem.\n",
        "* Assumes feature independence.\n",
        "* Performs well with text classification and high-dimensional data.\n",
        "\n",
        "### 6. **Neural Networks**\n",
        "\n",
        "* Can model complex non-linear relationships.\n",
        "* Suitable for large datasets.\n",
        "* Requires more computational resources and tuning.\n",
        "\n",
        "### 7. **Gradient Boosting Machines (e.g., XGBoost, LightGBM)**\n",
        "\n",
        "* Ensemble methods that build trees sequentially to correct errors.\n",
        "* Often achieve state-of-the-art performance.\n",
        "* Flexible and handles various data types.\n",
        "\n",
        "---\n",
        "\n",
        "When to Consider Alternatives:\n",
        "\n",
        "* When data is **non-linearly separable**.\n",
        "* When you want to capture **complex feature interactions**.\n",
        "* When interpretability is less important.\n",
        "* When you need better performance on large or high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "h-KPW9blRcyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. What are Classification Evaluation Metrics.\n",
        "\n",
        "**Classification evaluation metrics** are used to measure how well a classification model performs. They help you understand the model’s accuracy, error types, and overall effectiveness, especially when dealing with different kinds of data or imbalanced classes.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Classification Metrics:\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "\n",
        "* Percentage of correctly predicted instances out of all instances.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "* Good for balanced datasets but can be misleading if classes are imbalanced.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Precision**\n",
        "\n",
        "* Of all instances predicted as positive, how many are actually positive.\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Useful when **false positives** are costly.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* Of all actual positive instances, how many did the model correctly identify.\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "* Important when **false negatives** are costly.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **F1 Score**\n",
        "\n",
        "* Harmonic mean of precision and recall.\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "* Balances precision and recall, useful when classes are imbalanced.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Confusion Matrix**\n",
        "\n",
        "* A table showing counts of:\n",
        "\n",
        "  * True Positives (TP)\n",
        "  * True Negatives (TN)\n",
        "  * False Positives (FP)\n",
        "  * False Negatives (FN)\n",
        "* Provides a full picture of classification errors.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **ROC Curve and AUC (Area Under Curve)**\n",
        "\n",
        "* **ROC curve** plots True Positive Rate vs. False Positive Rate at various thresholds.\n",
        "* **AUC** measures overall ability of the model to discriminate between classes.\n",
        "* AUC ranges from 0.5 (random guess) to 1 (perfect).\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Log Loss (Cross-Entropy Loss)**\n",
        "\n",
        "* Measures the uncertainty of the predictions based on their probability.\n",
        "* Lower values are better.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ywstJK4sRcvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. How does class imbalance affect Logistic Regression.**\n",
        "\n",
        "Class imbalance can significantly affect the performance of **logistic regression** and other classification models. Here’s how:\n",
        "\n",
        "---\n",
        "\n",
        "### **Impact of Class Imbalance on Logistic Regression**\n",
        "\n",
        "1. **Bias Toward the Majority Class:**\n",
        "\n",
        "   * Logistic regression tries to minimize the overall error.\n",
        "   * When one class (majority) dominates, the model tends to predict the majority class more often to reduce error.\n",
        "   * This leads to **poor detection of the minority class**.\n",
        "\n",
        "2. **Misleading Accuracy:**\n",
        "\n",
        "   * Accuracy can be high simply by predicting the majority class every time.\n",
        "   * This hides the fact that the model fails to recognize the minority class correctly.\n",
        "\n",
        "3. **Poor Recall for Minority Class:**\n",
        "\n",
        "   * Since the model favors the majority, recall (sensitivity) for the minority class drops.\n",
        "   * This is critical if the minority class represents important events (e.g., fraud, disease).\n",
        "\n",
        "4. **Decision Boundary Skew:**\n",
        "\n",
        "   * The learned decision boundary is biased and less sensitive to the minority class.\n",
        "   * The predicted probabilities for the minority class tend to be lower, making it harder to detect.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Handle Class Imbalance in Logistic Regression**\n",
        "\n",
        "* **Resampling Techniques:**\n",
        "\n",
        "  * **Oversampling** the minority class (e.g., SMOTE).\n",
        "  * **Undersampling** the majority class.\n",
        "\n",
        "* **Use of Class Weights:**\n",
        "\n",
        "  * Assign higher penalty (weight) to misclassifying minority class.\n",
        "  * Many logistic regression implementations allow setting `class_weight='balanced'`.\n",
        "\n",
        "* **Threshold Adjustment:**\n",
        "\n",
        "  * Change the classification threshold from 0.5 to a lower value to increase sensitivity.\n",
        "\n",
        "* **Evaluation Metrics:**\n",
        "\n",
        "  * Use metrics like **Precision, Recall, F1-score, ROC-AUC** instead of accuracy.\n"
      ],
      "metadata": {
        "id": "TaJitA60RctP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.What is Hyperparameter Tuning in Logistic Regression.**\n",
        "\n",
        "**Hyperparameter tuning** in logistic regression is the process of selecting the best values for **hyperparameters**—parameters that control the learning process but are not learned from the data itself. Proper tuning improves the model’s performance and generalization on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Hyperparameters in Logistic Regression:\n",
        "\n",
        "1. **Regularization Parameter ($\\lambda$ or $C$)**\n",
        "\n",
        "   * Controls the strength of regularization.\n",
        "   * $\\lambda$ is often used in theory; in many libraries (e.g., scikit-learn), $C = \\frac{1}{\\lambda}$ represents inverse regularization strength.\n",
        "   * Higher $\\lambda$ (lower $C$) means stronger regularization (simpler model).\n",
        "   * Lower $\\lambda$ (higher $C$) means less regularization (more complex model).\n",
        "\n",
        "2. **Regularization Type**\n",
        "\n",
        "   * L1 (Lasso) or L2 (Ridge).\n",
        "   * L1 can do feature selection by shrinking some coefficients to zero.\n",
        "   * L2 shrinks coefficients but keeps all features.\n",
        "\n",
        "3. **Solver**\n",
        "\n",
        "   * Algorithm used to optimize the logistic regression cost function (e.g., ‘liblinear’, ‘lbfgs’, ‘saga’).\n",
        "   * Different solvers support different types of regularization or are better suited for large datasets.\n",
        "\n",
        "4. **Max Iterations**\n",
        "\n",
        "   * Number of iterations for the solver to converge.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Tune Hyperparameters?\n",
        "\n",
        "* To find the **best balance between bias and variance**.\n",
        "* To **avoid underfitting or overfitting**.\n",
        "* To **improve accuracy, precision, recall**, or other relevant metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Tune Hyperparameters?\n",
        "\n",
        "* **Grid Search:** Try all combinations of specified hyperparameter values.\n",
        "* **Random Search:** Try random combinations over specified distributions.\n",
        "* **Automated methods:** Bayesian optimization, genetic algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "In Python with scikit-learn, you might tune the regularization strength $C$ and penalty type using GridSearchCV:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(grid.best_params_)\n",
        "```\n"
      ],
      "metadata": {
        "id": "AYzrwIAuRcqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14. What are different solvers in Logistic Regression? Which one should be used.**\n",
        "\n",
        "In logistic regression, **solvers** are optimization algorithms used to find the best model parameters by minimizing the loss function. Different solvers have different strengths, limitations, and support for regularization types.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Solvers in Logistic Regression (e.g., in scikit-learn):\n",
        "\n",
        "1. **liblinear**\n",
        "\n",
        "   * Uses a coordinate descent algorithm.\n",
        "   * Works well for small to medium datasets.\n",
        "   * Supports both **L1** and **L2** regularization.\n",
        "   * Suitable for **binary classification**.\n",
        "   * Can be slower on large datasets.\n",
        "\n",
        "2. **lbfgs**\n",
        "\n",
        "   * Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm.\n",
        "   * A quasi-Newton method.\n",
        "   * Supports **only L2** regularization.\n",
        "   * Efficient for large datasets.\n",
        "   * Supports **multinomial logistic regression** (multiclass) natively.\n",
        "\n",
        "3. **newton-cg**\n",
        "\n",
        "   * Newton’s method with conjugate gradient optimization.\n",
        "   * Supports **only L2** regularization.\n",
        "   * Can handle multinomial logistic regression.\n",
        "   * Often efficient for medium to large datasets.\n",
        "\n",
        "4. **sag** (Stochastic Average Gradient)\n",
        "\n",
        "   * An optimization algorithm that uses stochastic gradients.\n",
        "   * Supports **only L2** regularization.\n",
        "   * Efficient for large datasets.\n",
        "   * Supports multinomial loss.\n",
        "\n",
        "5. **saga**\n",
        "\n",
        "   * Variant of SAG that supports **both L1 and L2** regularization.\n",
        "   * Efficient for large datasets.\n",
        "   * Supports **multinomial logistic regression**.\n",
        "   * Often recommended for large-scale problems with sparse data.\n",
        "\n",
        "---\n",
        "\n",
        "### Which Solver to Use?\n",
        "\n",
        "| Scenario                                                             | Recommended Solver                   |\n",
        "| -------------------------------------------------------------------- | ------------------------------------ |\n",
        "| Small to medium dataset, binary classification, need L1 or L2        | **liblinear**                        |\n",
        "| Large dataset, L2 regularization, multiclass classification          | **lbfgs**, **newton-cg**, or **sag** |\n",
        "| Large dataset, sparse data, need L1 or L2 regularization, multiclass | **saga**                             |\n",
        "| General-purpose, multiclass, efficient convergence                   | **lbfgs** or **saga**                |\n"
      ],
      "metadata": {
        "id": "m_0MczDHRcoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15. How is Logistic Regression extended for multiclass classification.**\n",
        "\n",
        "Logistic regression is naturally a **binary classifier**, but it can be extended to handle **multiclass classification** (more than two classes) using strategies that generalize the logistic function and model training.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Approaches to Multiclass Logistic Regression:\n",
        "\n",
        "#### 1. **One-vs-Rest (OvR) / One-vs-All**\n",
        "\n",
        "* Train **one binary logistic regression model per class**.\n",
        "* For each model, treat that class as the positive class and all others as negative.\n",
        "* At prediction, compute the probability from each model, then assign the class with the highest probability.\n",
        "* Simple and widely used.\n",
        "* Supported by many libraries as a default multiclass strategy.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Multinomial Logistic Regression (Softmax Regression)**\n",
        "\n",
        "* Generalizes logistic regression by directly modeling the probabilities of each class.\n",
        "* Uses the **softmax function** to convert raw scores (logits) into probabilities for each class.\n",
        "\n",
        "  For $K$ classes and feature vector $\\mathbf{x}$:\n",
        "\n",
        "  $$\n",
        "  P(Y = k \\mid \\mathbf{x}) = \\frac{e^{\\mathbf{w}_k^\\top \\mathbf{x}}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^\\top \\mathbf{x}}}\n",
        "  $$\n",
        "* The model jointly learns parameters for all classes.\n",
        "* Typically gives better performance and probability estimates compared to OvR.\n",
        "* Requires solvers that support multinomial loss (e.g., `lbfgs`, `newton-cg`, `saga`).\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Differences:\n",
        "\n",
        "| Approach          | Description                             | Pros                                           | Cons                                                          |\n",
        "| ----------------- | --------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------- |\n",
        "| One-vs-Rest (OvR) | Train one binary classifier per class   | Simple, scalable                               | Can produce inconsistent probabilities; suboptimal boundaries |\n",
        "| Multinomial       | Model all classes jointly using softmax | More accurate probabilities; jointly optimized | More computationally intensive                                |\n"
      ],
      "metadata": {
        "id": "sfY387TTRclb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.What are the advantages and disadvantages of Logistic Regression.**\n",
        "\n",
        "Here’s a clear overview of the **advantages and disadvantages of logistic regression**:\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Simple and Easy to Implement**\n",
        "\n",
        "   * Straightforward mathematical formulation.\n",
        "   * Fast to train and predict.\n",
        "\n",
        "2. **Interpretable Results**\n",
        "\n",
        "   * Coefficients indicate the influence of each feature.\n",
        "   * Outputs probabilities, not just class labels.\n",
        "\n",
        "3. **Effective for Binary Classification**\n",
        "\n",
        "   * Well-suited for problems where the outcome is categorical with two classes.\n",
        "\n",
        "4. **Probabilistic Output**\n",
        "\n",
        "   * Gives estimated probabilities, useful for decision-making and ranking.\n",
        "\n",
        "5. **Less Prone to Overfitting (with Regularization)**\n",
        "\n",
        "   * Regularization techniques (L1, L2) help control overfitting.\n",
        "\n",
        "6. **Works Well with Linearly Separable Data**\n",
        "\n",
        "   * Performs well when classes can be separated by a linear boundary.\n",
        "\n",
        "7. **Can Be Extended to Multiclass**\n",
        "\n",
        "   * Via one-vs-rest or multinomial logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Assumes Linear Relationship**\n",
        "\n",
        "   * Assumes linearity between independent variables and the log-odds of the outcome.\n",
        "   * Struggles with complex, non-linear relationships.\n",
        "\n",
        "2. **Sensitive to Outliers**\n",
        "\n",
        "   * Extreme values can distort the model.\n",
        "\n",
        "3. **Not Suitable for Complex Patterns**\n",
        "\n",
        "   * Cannot automatically capture feature interactions or non-linearities without manual feature engineering.\n",
        "\n",
        "4. **Requires Large Sample Size**\n",
        "\n",
        "   * Especially when the number of features is high to get stable and reliable estimates.\n",
        "\n",
        "5. **Poor Performance on Imbalanced Data**\n",
        "\n",
        "   * Without adjustments, it tends to be biased toward the majority class.\n",
        "\n",
        "6. **Limited to Classification Tasks**\n",
        "\n",
        "   * Not designed for regression or continuous output prediction.\n"
      ],
      "metadata": {
        "id": "SE8XZycdRcjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.What are some use cases of Logistic Regression.**\n",
        "\n",
        "Here’s a clear overview of the **advantages and disadvantages of logistic regression**:\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "1. **Simple and Easy to Implement**\n",
        "\n",
        "   * Straightforward mathematical formulation.\n",
        "   * Fast to train and predict.\n",
        "\n",
        "2. **Interpretable Results**\n",
        "\n",
        "   * Coefficients indicate the influence of each feature.\n",
        "   * Outputs probabilities, not just class labels.\n",
        "\n",
        "3. **Effective for Binary Classification**\n",
        "\n",
        "   * Well-suited for problems where the outcome is categorical with two classes.\n",
        "\n",
        "4. **Probabilistic Output**\n",
        "\n",
        "   * Gives estimated probabilities, useful for decision-making and ranking.\n",
        "\n",
        "5. **Less Prone to Overfitting (with Regularization)**\n",
        "\n",
        "   * Regularization techniques (L1, L2) help control overfitting.\n",
        "\n",
        "6. **Works Well with Linearly Separable Data**\n",
        "\n",
        "   * Performs well when classes can be separated by a linear boundary.\n",
        "\n",
        "7. **Can Be Extended to Multiclass**\n",
        "\n",
        "   * Via one-vs-rest or multinomial logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "1. **Assumes Linear Relationship**\n",
        "\n",
        "   * Assumes linearity between independent variables and the log-odds of the outcome.\n",
        "   * Struggles with complex, non-linear relationships.\n",
        "\n",
        "2. **Sensitive to Outliers**\n",
        "\n",
        "   * Extreme values can distort the model.\n",
        "\n",
        "3. **Not Suitable for Complex Patterns**\n",
        "\n",
        "   * Cannot automatically capture feature interactions or non-linearities without manual feature engineering.\n",
        "\n",
        "4. **Requires Large Sample Size**\n",
        "\n",
        "   * Especially when the number of features is high to get stable and reliable estimates.\n",
        "\n",
        "5. **Poor Performance on Imbalanced Data**\n",
        "\n",
        "   * Without adjustments, it tends to be biased toward the majority class.\n",
        "\n",
        "6. **Limited to Classification Tasks**\n",
        "\n",
        "   * Not designed for regression or continuous output prediction.\n"
      ],
      "metadata": {
        "id": "3_kaGk8yRcgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18. What is the difference between Softmax Regression and Logistic Regression.**\n",
        "\n",
        "Logistic regression is widely used for **classification tasks**, especially when the outcome is binary or categorical. Here are some common and practical **use cases** across industries:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Healthcare**\n",
        "\n",
        "* **Disease Prediction**: Predicting whether a patient has a disease (e.g., diabetes, heart disease) based on risk factors.\n",
        "* **Readmission Risk**: Estimating the probability of a patient being readmitted to the hospital.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Finance**\n",
        "\n",
        "* **Credit Scoring**: Predicting whether a customer will default on a loan or credit card.\n",
        "* **Fraud Detection**: Classifying transactions as fraudulent or not based on transaction behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Marketing**\n",
        "\n",
        "* **Customer Churn Prediction**: Determining if a customer is likely to stop using a service.\n",
        "* **Campaign Effectiveness**: Predicting whether a user will respond to an advertisement or email.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Human Resources**\n",
        "\n",
        "* **Hiring Decisions**: Predicting the likelihood of a candidate accepting an offer or performing well.\n",
        "* **Employee Attrition**: Identifying employees at risk of leaving the company.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Retail / E-commerce**\n",
        "\n",
        "* **Purchase Prediction**: Estimating whether a visitor will complete a purchase.\n",
        "* **Product Recommendation (as a classifier)**: Predicting whether a customer will like or click on a specific product.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Social Media and Tech**\n",
        "\n",
        "* **Spam Detection**: Classifying messages or emails as spam or not.\n",
        "* **User Behavior Prediction**: Predicting whether a user will click, subscribe, or engage with content.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Law and Policy**\n",
        "\n",
        "* **Recidivism Prediction**: Estimating the likelihood that an individual will re-offend.\n",
        "* **Survey Analysis**: Predicting opinions or preferences from demographics or previous responses.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Logistic Regression is Often Chosen:\n",
        "\n",
        "* Interpretable results (important in regulated fields).\n",
        "* Works well with structured data.\n",
        "* Fast to train and deploy for real-time scoring.\n",
        "\n"
      ],
      "metadata": {
        "id": "I1yIzY52RceN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.**\n",
        "\n",
        "Choosing between **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** depends on several factors such as model performance, interpretability, dataset size, and implementation needs.\n",
        "\n",
        "Here’s a comparison to help you decide:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Type and Behavior**\n",
        "\n",
        "| Criterion               | One-vs-Rest (OvR)                          | Softmax (Multinomial)                 |\n",
        "| ----------------------- | ------------------------------------------ | ------------------------------------- |\n",
        "| Strategy                | Trains one binary classifier per class     | Trains a single model for all classes |\n",
        "| Decision Mechanism      | Selects class with highest binary score    | Uses softmax to assign probabilities  |\n",
        "| Probability Consistency | May give inconsistent or overlapping probs | Probabilities sum to 1 (normalized)   |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Accuracy and Performance**\n",
        "\n",
        "* **Softmax** typically gives **better calibrated probabilities** and **more optimal class boundaries**, especially when classes are closely related or overlap.\n",
        "* **OvR** can perform similarly if the data is clean and separable, but it's more prone to **inconsistencies** between binary classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Computation and Scalability**\n",
        "\n",
        "| Factor           | OvR                                | Softmax                                             |\n",
        "| ---------------- | ---------------------------------- | --------------------------------------------------- |\n",
        "| Number of models | One model per class                | One unified model                                   |\n",
        "| Training time    | Slightly faster for small datasets | Better for large-scale learning with modern solvers |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Support in Libraries**\n",
        "\n",
        "* Both are supported in libraries like **scikit-learn**.\n",
        "* In `LogisticRegression`, use:\n",
        "\n",
        "  * `multi_class='ovr'` for One-vs-Rest\n",
        "  * `multi_class='multinomial'` for Softmax\n",
        "  * Use solvers like `'lbfgs'`, `'newton-cg'`, or `'saga'` for multinomial\n",
        "\n",
        "---\n",
        "\n",
        "### **5. When to Choose What**\n",
        "\n",
        "| Use Case                                             | Recommended Approach  |\n",
        "| ---------------------------------------------------- | --------------------- |\n",
        "| Small dataset, high interpretability needed          | One-vs-Rest           |\n",
        "| Accurate probability estimates needed                | Softmax (Multinomial) |\n",
        "| Closely related classes with overlaps                | Softmax               |\n",
        "| Computational constraints, binary-only model support | One-vs-Rest           |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zsKmDr7TRcbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "In **logistic regression**, coefficients represent the relationship between each **independent variable** and the **log-odds** of the target class. Here’s how to interpret them step by step:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Coefficient Basics**\n",
        "\n",
        "* Let’s say you have the logistic regression equation:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p$ = probability of the positive class\n",
        "* $\\beta_0$ = intercept\n",
        "* $\\beta_i$ = coefficient of feature $x_i$\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Interpretation in Terms of Log-Odds**\n",
        "\n",
        "* Each coefficient $\\beta_i$ is the **change in the log-odds** of the outcome for a one-unit increase in $x_i$, **holding all other variables constant**.\n",
        "\n",
        "Example:\n",
        "If $\\beta_2 = 0.7$, then increasing $x_2$ by 1 increases the **log-odds** of the positive class by 0.7.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Interpretation in Terms of Odds Ratio**\n",
        "\n",
        "To make interpretation more intuitive, **exponentiate** the coefficient:\n",
        "\n",
        "$$\n",
        "\\text{Odds Ratio} = e^{\\beta_i}\n",
        "$$\n",
        "\n",
        "* $e^{\\beta_i} > 1$: positive effect on the outcome\n",
        "* $e^{\\beta_i} < 1$: negative effect\n",
        "* $e^{\\beta_i} = 1$: no effect\n",
        "\n",
        "Example:\n",
        "If $\\beta_2 = 0.7$, then $e^{0.7} \\approx 2.01$, meaning the odds of the outcome **double** for each unit increase in $x_2$.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Sign of the Coefficient**\n",
        "\n",
        "* **Positive coefficient**: increases the probability of the positive class.\n",
        "* **Negative coefficient**: decreases the probability of the positive class.\n",
        "* **Zero coefficient**: no impact.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Categorical Variables**\n",
        "\n",
        "* For categorical variables (after one-hot encoding), coefficients represent the impact of being in that category **compared to the reference category**.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Intercept ($\\beta_0$)**\n",
        "\n",
        "* The log-odds of the outcome when **all features are zero**.\n",
        "* Not usually interpretable on its own unless zero is a meaningful value for all predictors.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kiSIenK5RcZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Practical Quesions**"
      ],
      "metadata": {
        "id": "hcFBxKCvn6Va"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu5xMzU2RQ3W",
        "outputId": "e2094351-597a-40fe-ae70-f1c042cf1b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Q01.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q02. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L1 Regularization Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TLkNx96og9x",
        "outputId": "d89c7aa1-9575-43bb-dad4-3260c8b7107b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L1 Regularization Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q03.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L2 Regularization Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature_name, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature_name}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBuUB1DEo6ZO",
        "outputId": "73b65933-5cd1-4845-cd52-d4c71f8b1421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 Regularization Accuracy: 0.96\n",
            "Model Coefficients:\n",
            "mean radius: 2.1325\n",
            "mean texture: 0.1528\n",
            "mean perimeter: -0.1451\n",
            "mean area: -0.0008\n",
            "mean smoothness: -0.1426\n",
            "mean compactness: -0.4156\n",
            "mean concavity: -0.6519\n",
            "mean concave points: -0.3445\n",
            "mean symmetry: -0.2076\n",
            "mean fractal dimension: -0.0298\n",
            "radius error: -0.0500\n",
            "texture error: 1.4430\n",
            "perimeter error: -0.3039\n",
            "area error: -0.0726\n",
            "smoothness error: -0.0162\n",
            "compactness error: -0.0019\n",
            "concavity error: -0.0449\n",
            "concave points error: -0.0377\n",
            "symmetry error: -0.0418\n",
            "fractal dimension error: 0.0056\n",
            "worst radius: 1.2321\n",
            "worst texture: -0.4046\n",
            "worst perimeter: -0.0362\n",
            "worst area: -0.0271\n",
            "worst smoothness: -0.2626\n",
            "worst compactness: -1.2090\n",
            "worst concavity: -1.6180\n",
            "worst concave points: -0.6153\n",
            "worst symmetry: -0.7428\n",
            "worst fractal dimension: -0.1170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q04. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model with Elastic Net Regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',          # 'saga' solver is required for elasticnet\n",
        "    l1_ratio=0.5,           # 0.5 = equal mix of L1 and L2\n",
        "    C=1.0,                  # Regularization strength (lower = stronger regularization)\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with Elastic Net Regularization Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature_name, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature_name}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VCqx-TapgDI",
        "outputId": "d0dc39ca-4afc-498b-b5bb-a759e4f5020a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Elastic Net Regularization Accuracy: 0.96\n",
            "Model Coefficients:\n",
            "mean radius: 0.0090\n",
            "mean texture: 0.0100\n",
            "mean perimeter: 0.0509\n",
            "mean area: 0.0158\n",
            "mean smoothness: 0.0000\n",
            "mean compactness: -0.0001\n",
            "mean concavity: -0.0002\n",
            "mean concave points: -0.0001\n",
            "mean symmetry: 0.0001\n",
            "mean fractal dimension: 0.0000\n",
            "radius error: 0.0001\n",
            "texture error: 0.0009\n",
            "perimeter error: -0.0000\n",
            "area error: -0.0135\n",
            "smoothness error: 0.0000\n",
            "compactness error: 0.0000\n",
            "concavity error: -0.0000\n",
            "concave points error: 0.0000\n",
            "symmetry error: 0.0000\n",
            "fractal dimension error: 0.0000\n",
            "worst radius: 0.0094\n",
            "worst texture: 0.0111\n",
            "worst perimeter: 0.0490\n",
            "worst area: -0.0245\n",
            "worst smoothness: 0.0001\n",
            "worst compactness: -0.0003\n",
            "worst concavity: -0.0005\n",
            "worst concave points: -0.0001\n",
            "worst symmetry: 0.0001\n",
            "worst fractal dimension: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q05. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multiclass Logistic Regression (OvR) Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfOolxt_q15w",
        "outputId": "0db48422-470c-41aa-de62-2cffd61634f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q06.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of LogisticRegression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define Logistic Regression and parameter grid\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],             # Regularization strength\n",
        "    'penalty': ['l1', 'l2']                   # L1 (Lasso), L2 (Ridge)\n",
        "}\n",
        "\n",
        "# Step 4: GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 6: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "39eFAU1oqHnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373d255f-8182-4179-af2c-bf1bae0c84c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Test Accuracy with Best Model: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Define the model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Step 3: Define Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Evaluate model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "print(f\"Average Accuracy: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Jut9ZcquFZ",
        "outputId": "6add5038-afe1-4ed8-cb8c-517e568c6151"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.94736842 0.92105263 0.95614035 0.96491228 0.96460177]\n",
            "Average Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a simple example CSV file with 3 features and a binary target\n",
        "data = {\n",
        "    'feature1': [5, 7, 8, 2, 1, 6, 9, 4, 3, 7],\n",
        "    'feature2': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "    'feature3': [10, 15, 10, 7, 5, 9, 11, 6, 7, 12],\n",
        "    'target':   [0, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('example_data.csv', index=False)\n",
        "print(\"CSV file 'example_data.csv' created.\")\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from CSV file\n",
        "df = pd.read_csv('example_data.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBNRup2gqt9Q",
        "outputId": "b63d9525-31e6-4aca-81e2-99f7cdd61b70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'example_data.csv' created.\n",
            "Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q09. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Step 4: Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),  # Continuous uniform distribution between 0.01 and 10.01\n",
        "    'penalty': ['l1', 'l2'],            # L1 and L2 penalties\n",
        "    'solver': ['liblinear', 'saga']     # Solvers supporting these penalties\n",
        "}\n",
        "\n",
        "# Step 5: RandomizedSearchCV setup\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,               # Number of parameter settings sampled\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU_o2kRgqtwQ",
        "outputId": "df54b3ac-b059-4d0f-a603-1ba5453c1760"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(7.3299394181140505), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Test Accuracy with Best Model: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "base_model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Wrap the model with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z743fG0QqtjN",
        "outputId": "e851fd4f-cb29-4554-f8e1-d40df881002a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "6qON7EX3qtO9",
        "outputId": "1a859302-285b-4ac0-8e12-343e8b18653e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAHWCAYAAAB0TPAHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQohJREFUeJzt3XlclXX6//H3AeGIIJvJVq5pqKOVYqNkuWWZmUlQZsuIS1M5mgtqZt+pzCYpWjTNJcvULLU0Y0pLx50sNSMt28g1chRcSsGFA8L9+8OfZzqCxtGDN9zn9ZzH/XjE596uc4a6uK77c9+3zTAMQwAAwHJ8zA4AAABUDJI8AAAWRZIHAMCiSPIAAFgUSR4AAIsiyQMAYFEkeQAALIokDwCARZHkAQCwKJI8UE7bt2/XLbfcopCQENlsNqWnp3v0+Hv27JHNZtPs2bM9etyqrGPHjurYsaPZYQBVFkkeVcrOnTv18MMPq2HDhqpevbqCg4PVrl07vfrqqzp58mSFnjs5OVnbtm3Tc889p7lz56p169YVer5LqW/fvrLZbAoODi7ze9y+fbtsNptsNpteeuklt4+/b98+jR07Vlu3bvVAtADKq5rZAQDltXTpUt19992y2+3q06ePmjdvrsLCQq1fv16jRo3S999/rxkzZlTIuU+ePKkNGzbo//7v/zR48OAKOUe9evV08uRJ+fn5Vcjx/0y1atV04sQJffzxx+rVq5fLunfffVfVq1dXQUHBBR173759euaZZ1S/fn1de+215d7vP//5zwWdD8BpJHlUCbt371bv3r1Vr149rV69WtHR0c51gwYN0o4dO7R06dIKO//BgwclSaGhoRV2DpvNpurVq1fY8f+M3W5Xu3btNH/+/FJJft68eerevbs++OCDSxLLiRMnVKNGDfn7+1+S8wFWRbseVUJaWpqOHTummTNnuiT4Mxo1aqShQ4c6fz516pSeffZZXXnllbLb7apfv76eeOIJORwOl/3q16+v22+/XevXr9df//pXVa9eXQ0bNtTbb7/t3Gbs2LGqV6+eJGnUqFGy2WyqX7++pNNt7jP//Edjx46VzWZzGVuxYoVuuOEGhYaGKigoSLGxsXriiSec6891TX716tW68cYbFRgYqNDQUPXs2VM//vhjmefbsWOH+vbtq9DQUIWEhKhfv346ceLEub/Ys9x333369NNPdeTIEefY5s2btX37dt13332ltv/tt980cuRItWjRQkFBQQoODla3bt30zTffOLdZu3atrrvuOklSv379nG3/M5+zY8eOat68uTIzM9W+fXvVqFHD+b2cfU0+OTlZ1atXL/X5u3btqrCwMO3bt6/cnxXwBiR5VAkff/yxGjZsqOuvv75c2z/44IN66qmn1KpVK02YMEEdOnRQamqqevfuXWrbHTt26K677tLNN9+sl19+WWFhYerbt6++//57SVJiYqImTJggSbr33ns1d+5cTZw40a34v//+e91+++1yOBwaN26cXn75Zd1xxx36/PPPz7vfypUr1bVrVx04cEBjx45VSkqKvvjiC7Vr10579uwptX2vXr2Un5+v1NRU9erVS7Nnz9YzzzxT7jgTExNls9m0ePFi59i8efPUpEkTtWrVqtT2u3btUnp6um6//Xa98sorGjVqlLZt26YOHTo4E27Tpk01btw4SdJDDz2kuXPnau7cuWrfvr3zOIcPH1a3bt107bXXauLEierUqVOZ8b366quqXbu2kpOTVVxcLEl6/fXX9Z///EeTJ09WTExMuT8r4BUMoJI7evSoIcno2bNnubbfunWrIcl48MEHXcZHjhxpSDJWr17tHKtXr54hycjIyHCOHThwwLDb7caIESOcY7t37zYkGS+++KLLMZOTk4169eqViuHpp582/viv14QJEwxJxsGDB88Z95lzzJo1yzl27bXXGhEREcbhw4edY998843h4+Nj9OnTp9T5+vfv73LMO++806hVq9Y5z/nHzxEYGGgYhmHcddddxk033WQYhmEUFxcbUVFRxjPPPFPmd1BQUGAUFxeX+hx2u90YN26cc2zz5s2lPtsZHTp0MCQZ06dPL3Ndhw4dXMaWL19uSDL+9a9/Gbt27TKCgoKMhISEP/2MgDeikkell5eXJ0mqWbNmubb/5JNPJEkpKSku4yNGjJCkUtfumzVrphtvvNH5c+3atRUbG6tdu3ZdcMxnO3Mt/9///rdKSkrKtc/+/fu1detW9e3bV+Hh4c7xq6++WjfffLPzc/7RI4884vLzjTfeqMOHDzu/w/K47777tHbtWuXk5Gj16tXKyckps1Uvnb6O7+Nz+j8jxcXFOnz4sPNSxNdff13uc9rtdvXr169c295yyy16+OGHNW7cOCUmJqp69ep6/fXXy30uwJuQ5FHpBQcHS5Ly8/PLtf0vv/wiHx8fNWrUyGU8KipKoaGh+uWXX1zG69atW+oYYWFh+v333y8w4tLuuecetWvXTg8++KAiIyPVu3dvvf/+++dN+GfijI2NLbWuadOmOnTokI4fP+4yfvZnCQsLkyS3Psttt92mmjVr6r333tO7776r6667rtR3eUZJSYkmTJigxo0by26367LLLlPt2rX17bff6ujRo+U+5+WXX+7WJLuXXnpJ4eHh2rp1qyZNmqSIiIhy7wt4E5I8Kr3g4GDFxMTou+++c2u/sye+nYuvr2+Z44ZhXPA5zlwvPiMgIEAZGRlauXKl/va3v+nbb7/VPffco5tvvrnUthfjYj7LGXa7XYmJiZozZ44+/PDDc1bxkjR+/HilpKSoffv2euedd7R8+XKtWLFCf/nLX8rdsZBOfz/u2LJliw4cOCBJ2rZtm1v7At6EJI8q4fbbb9fOnTu1YcOGP922Xr16Kikp0fbt213Gc3NzdeTIEedMeU8ICwtzmYl+xtndAkny8fHRTTfdpFdeeUU//PCDnnvuOa1evVpr1qwp89hn4szKyiq17qefftJll12mwMDAi/sA53Dfffdpy5Ytys/PL3Oy4hmLFi1Sp06dNHPmTPXu3Vu33HKLunTpUuo7Ke8fXOVx/Phx9evXT82aNdNDDz2ktLQ0bd682WPHB6yEJI8q4bHHHlNgYKAefPBB5ebmllq/c+dOvfrqq5JOt5sllZoB/8orr0iSunfv7rG4rrzySh09elTffvutc2z//v368MMPXbb77bffSu175qEwZ9/Wd0Z0dLSuvfZazZkzxyVpfvfdd/rPf/7j/JwVoVOnTnr22Wf12muvKSoq6pzb+fr6luoSLFy4UP/9739dxs78MVLWH0TuGj16tLKzszVnzhy98sorql+/vpKTk8/5PQLejIfhoEq48sorNW/ePN1zzz1q2rSpyxPvvvjiCy1cuFB9+/aVJF1zzTVKTk7WjBkzdOTIEXXo0EFffvml5syZo4SEhHPennUhevfurdGjR+vOO+/UkCFDdOLECU2bNk1XXXWVy8SzcePGKSMjQ927d1e9evV04MABTZ06VVdccYVuuOGGcx7/xRdfVLdu3RQfH68BAwbo5MmTmjx5skJCQjR27FiPfY6z+fj46J///Oefbnf77bdr3Lhx6tevn66//npt27ZN7777rho2bOiy3ZVXXqnQ0FBNnz5dNWvWVGBgoNq0aaMGDRq4Fdfq1as1depUPf30085b+mbNmqWOHTvqySefVFpamlvHAyzP5Nn9gFt+/vln4+9//7tRv359w9/f36hZs6bRrl07Y/LkyUZBQYFzu6KiIuOZZ54xGjRoYPj5+Rl16tQxxowZ47KNYZy+ha579+6lznP2rVvnuoXOMAzjP//5j9G8eXPD39/fiI2NNd55551St9CtWrXK6NmzpxETE2P4+/sbMTExxr333mv8/PPPpc5x9m1mK1euNNq1a2cEBAQYwcHBRo8ePYwffvjBZZsz5zv7Fr1Zs2YZkozdu3ef8zs1DNdb6M7lXLfQjRgxwoiOjjYCAgKMdu3aGRs2bCjz1rd///vfRrNmzYxq1aq5fM4OHToYf/nLX8o85x+Pk5eXZ9SrV89o1aqVUVRU5LLd8OHDDR8fH2PDhg3n/QyAt7EZhhszcgAAQJXBNXkAACyKJA8AgEWR5AEAsCiSPAAAl1j9+vWdb2T84zJo0CBJUkFBgQYNGqRatWopKChISUlJZd4+/GeYeAcAwCV28OBBl6ddfvfdd7r55pu1Zs0adezYUQMHDtTSpUs1e/ZshYSEaPDgwfLx8fnTN1eejSQPAIDJhg0bpiVLlmj79u3Ky8tT7dq1NW/ePN11112STj/lsmnTptqwYYPatm1b7uPSrgcAwAMcDofy8vJclvI8ibGwsFDvvPOO+vfvL5vNpszMTBUVFalLly7ObZo0aaK6deuW69Hef2TJJ9498M43ZocAVLjJic3NDgGocGE1yn7pkqcEtBzssWON7nmZnnnmGZexp59++k+fTpmenq4jR444n9qZk5Mjf39/5yuqz4iMjFROTo5bMVkyyQMAUC42zzW0x4wZo5SUFJcxu93+p/vNnDlT3bp1U0xMjMdiOYMkDwCAB9jt9nIl9T/65ZdftHLlSi1evNg5FhUVpcLCQh05csSlms/NzT3vC6PKwjV5AID3stk8t1yAWbNmKSIiwuXtmHFxcfLz89OqVaucY1lZWcrOzlZ8fLxbx6eSBwB4Lw+2691VUlKiWbNmKTk5WdWq/S8dh4SEaMCAAUpJSVF4eLiCg4P16KOPKj4+3q2Z9RJJHgAAU6xcuVLZ2dnq379/qXUTJkyQj4+PkpKS5HA41LVrV02dOtXtc1jyPnlm18MbMLse3qDCZ9dfl/LnG5XTyc2veOxYnkIlDwDwXia26y8Fa386AAC8GJU8AMB7XeCs+KqCJA8A8F606wEAQFVEJQ8A8F606wEAsCja9QAAoCqikgcAeC/a9QAAWBTtegAAUBVRyQMAvBftegAALIp2PQAAqIqo5AEA3svilTxJHgDgvXysfU3e2n/CAADgxajkAQDei3Y9AAAWZfFb6Kz9JwwAAF6MSh4A4L1o1wMAYFG06wEAQFVEJQ8A8F606wEAsCja9QAAoCqikgcAeC/a9QAAWBTtegAAUBVRyQMAvBftegAALIp2PQAAqIqo5AEA3ot2PQAAFmXxJG/tTwcAgBejkgcAeC+LT7wjyQMAvBftegAAUBVRyQMAvBftegAALIp2PQAAqIqo5AEA3ot2PQAA1mSzeJKnXQ8AgEVRyQMAvBaVPAAAVmXz4OKm//73v3rggQdUq1YtBQQEqEWLFvrqq6+c6w3D0FNPPaXo6GgFBASoS5cu2r59u1vnIMkDAHCJ/f7772rXrp38/Pz06aef6ocfftDLL7+ssLAw5zZpaWmaNGmSpk+frk2bNikwMFBdu3ZVQUFBuc9Dux4A4LXMate/8MILqlOnjmbNmuUca9CggfOfDcPQxIkT9c9//lM9e/aUJL399tuKjIxUenq6evfuXa7zUMkDALyWzWbz2OJwOJSXl+eyOByOMs/70UcfqXXr1rr77rsVERGhli1b6o033nCu3717t3JyctSlSxfnWEhIiNq0aaMNGzaU+/OR5AEA8IDU1FSFhIS4LKmpqWVuu2vXLk2bNk2NGzfW8uXLNXDgQA0ZMkRz5syRJOXk5EiSIiMjXfaLjIx0risP2vUAAK/lyXb9mDFjlJKS4jJmt9vL3LakpEStW7fW+PHjJUktW7bUd999p+nTpys5OdljMVHJAwC8lifb9Xa7XcHBwS7LuZJ8dHS0mjVr5jLWtGlTZWdnS5KioqIkSbm5uS7b5ObmOteVB0keAIBLrF27dsrKynIZ+/nnn1WvXj1JpyfhRUVFadWqVc71eXl52rRpk+Lj48t9Htr1AADvZdKzcIYPH67rr79e48ePV69evfTll19qxowZmjFjxumwbDYNGzZM//rXv9S4cWM1aNBATz75pGJiYpSQkFDu85DkAQBey6xb6K677jp9+OGHGjNmjMaNG6cGDRpo4sSJuv/++53bPPbYYzp+/LgeeughHTlyRDfccIOWLVum6tWrl/s8NsMwjIr4AGZ64J1vzA4BqHCTE5ubHQJQ4cJq+Fbo8UPvf8djxzry7gMeO5anUMkDALyW1Z9dT5IHAHgtqyd5ZtcDAGBRVPIAAK9l9UqeJA8A8F7WzvG06wEAsCoqeQCA16JdX4EOHTqkt956Sxs2bHC+VScqKkrXX3+9+vbtq9q1a5sZHgDA4qye5E1r12/evFlXXXWVJk2apJCQELVv317t27dXSEiIJk2apCZNmuirr74yKzwAAKo80yr5Rx99VHfffbemT59e6i8pwzD0yCOP6NFHH9WGDRtMihAAYHVWr+RNS/LffPONZs+eXeYXbLPZNHz4cLVs2dKEyAAAXsPaOd68dn1UVJS+/PLLc67/8ssvFRkZeQkjAgDAWkyr5EeOHKmHHnpImZmZuummm5wJPTc3V6tWrdIbb7yhl156yazwAABegHZ9BRk0aJAuu+wyTZgwQVOnTlVxcbEkydfXV3FxcZo9e7Z69eplVngAAC9Akq9A99xzj+655x4VFRXp0KFDkqTLLrtMfn5+ZoYFAIAlVIqH4fj5+Sk6OtrsMAAAXoZKHgAAi7J6kufZ9QAAWBSVPADAe1m7kCfJAwC8l9Xb9aYk+Y8++qjc295xxx0VGAkAANZlSpJPSEgo13Y2m815/zwAAJ5GJV8BSkpKzDgtAAAurJ7kmV0PAIBFVYqJd8ePH9e6deuUnZ2twsJCl3VDhgwxKSoAgOVZu5A3P8lv2bJFt912m06cOKHjx48rPDxchw4dUo0aNRQREUGSBwBUGNr1FWz48OHq0aOHfv/9dwUEBGjjxo365ZdfFBcXx1voAAC4CKYn+a1bt2rEiBHy8fGRr6+vHA6H6tSpo7S0ND3xxBNmhwcAsDCbzeaxpTIyvV3v5+cnH5/Tf2tEREQoOztbTZs2VUhIiH799VeTo8Mf3dS4lm66qpZqB/pLkvYeLdCH23L17b58SVJEkL/uaxWjqyIC5edj07f78zVn83+VV3DKzLABj3r7rTc0dfIE3XPf3zR81Bizw8FFqqzJ2VNMT/ItW7bU5s2b1bhxY3Xo0EFPPfWUDh06pLlz56p58+Zmh4c/+O1Ekd7bsl85+Q7ZJN3YMFwpHerr/z75WYeOFWn0TQ2V/ftJjV+5U5J01zVRGtGxgcYu2y7D3NABj/jh+2368IP31ahxrNmhAOViert+/PjxztfMPvfccwoLC9PAgQN18OBBzZgxw+To8Edb/punb/blKze/UDn5hVr4TY4KTpWo0WWBahxRQ7UD/TVjw6/ae6RAe48U6PUvstWgVoCaRQWZHTpw0U6cOK6nn3hMY558RjWDg80OBx5Cu76CtW7d2vnPERERWrZsmYnRoLxsNqlN3VDZq/lo+6Hjigyyy5BUVPy/mr2o2JBhSLERgfo+55h5wQIe8FLqv9Tuxg76a9vrNevN180OB55SOXOzx5ie5C+Ww+GQw+FwGSsuKpSvn79JEVnbFaHVNbZrI/n5+qjgVIkmrtujfUcdyi84JcepEvVuGa33t+6XTTbd0zJavj42hQb4mR02cFFWLPtEWT/9oLfeed/sUAC3mJ7kGzRocN42x65du867f2pqqp555hmXsRZ3PqyrEwd6JD642p/n0P8t/VkB/r76a90QPXx9Xf1rxQ7tO+rQpM/2qN9fr9AtTS6TYUgb9vyu3YdPqMTgijyqrtyc/XrlxVRNmvam7Ha72eHAwyprm91TbIZh7n+BX331VZefi4qKtGXLFi1btkyjRo3S448/ft79y6rkH/4gi0r+Enn8poY6cKxQb23a6xwLsvuqpMTQiaISvZbUTJ/+eFBLfzhoYpTWNDmRiamXwro1KzU6ZYh8fX2dY8XFxbLZbPLx8VHGpq0u6+BZYTUq9ru9csSnHjvWzpe7eexYnmJ6JT906NAyx6dMmaKvvvrqT/e32+2l/romwV86NptUzcf1L+FjjtNvDmwWGaTg6tX09d48M0IDPKL1X+P17sJ/u4z96+n/U70GDfS3vg+S4FGpmZ7kz6Vbt24aM2aMZs2aZXYo+P96XRulb/bl6/DxQlX389X19UPVNDJIaatOX1Jp3zBM/807fX2+ce0aeqD15Vr240Htz3P8yZGByiswMFBXNmrsMlY9IEAhIaGlxlH1WLxbX3mT/KJFixQeHm52GPiD4OrV9Mj1dRUaUE0nior16+8FSlu1S9/9/5nz0cHV1atltIL8fXXweJE++i5Xn/54yOSoAeDcrH5N3vQk37JlS5cv2TAM5eTk6ODBg5o6daqJkeFsb27ce971723dr/e27r9E0QDmmfbmHLNDAMrF9CTfs2dPlyTv4+Oj2rVrq2PHjmrSpImJkQEArM7ihbz5SX7s2LFmhwAA8FJWb9eb/lhbX19fHThwoNT44cOHmbUKAMBFML2SP9dt+g6HQ/7+3AoHAKg4Fi/kzUvykyZNknS6VfLmm28qKOh/LzEpLi5WRkYG1+QBABXKx8faWd60JD9hwgRJpyv56dOnu7Tm/f39Vb9+fU2fPt2s8AAAqDBjx44t9Uj22NhY/fTTT5KkgoICjRgxQgsWLJDD4VDXrl01depURUZGunUe05L87t27JUmdOnXS4sWLFRYWZlYoAAAvZWa7/i9/+YtWrlzp/Llatf+l5OHDh2vp0qVauHChQkJCNHjwYCUmJurzzz936xymX5Nfs2aN2SEAAHDJVatWTVFRUaXGjx49qpkzZ2revHnq3LmzJGnWrFlq2rSpNm7cqLZt25b7HKbPrk9KStILL7xQajwtLU133323CREBALyFzWbz2OJwOJSXl+eynP0CtT/avn27YmJi1LBhQ91///3Kzs6WJGVmZqqoqEhdunRxbtukSRPVrVtXGzZscOvzmZ7kMzIydNttt5Ua79atmzIyMkyICADgLWw2zy2pqakKCQlxWVJTU8s8b5s2bTR79mwtW7ZM06ZN0+7du3XjjTcqPz9fOTk58vf3V2hoqMs+kZGRysnJcevzmd6uP3bsWJm3yvn5+Skvj7eXAQCqhjFjxiglJcVl7Oy3pJ7Rrdv/Xkt79dVXq02bNqpXr57ef/99BQQEeCwm0yv5Fi1a6L333is1vmDBAjVr1syEiAAA3sKT7Xq73a7g4GCX5VxJ/myhoaG66qqrtGPHDkVFRamwsFBHjhxx2SY3N7fMa/jnY3ol/+STTyoxMVE7d+50TjBYtWqV5s+fr4ULF5ocHQDAyirLY22PHTumnTt36m9/+5vi4uLk5+enVatWKSkpSZKUlZWl7OxsxcfHu3Vc05N8jx49lJ6ervHjx2vRokUKCAjQ1VdfrZUrV6pDhw5mhwcAgMeNHDlSPXr0UL169bRv3z49/fTT8vX11b333quQkBANGDBAKSkpCg8PV3BwsB599FHFx8e7NbNeqgRJXpK6d++u7t27lxr/7rvv1Lx5cxMiAgB4A7MK+b179+ree+/V4cOHVbt2bd1www3auHGjateuLen0A+N8fHyUlJTk8jAcd1WKJP9H+fn5mj9/vt58801lZmaquLjY7JAAABZlVrt+wYIF511fvXp1TZkyRVOmTLmo85g+8e6MjIwM9enTR9HR0XrppZfUuXNnbdy40eywAACoskyt5HNycjR79mzNnDlTeXl56tWrlxwOh9LT05lZDwCocJVk3l2FMa2S79Gjh2JjY/Xtt99q4sSJ2rdvnyZPnmxWOAAAL+TJW+gqI9Mq+U8//VRDhgzRwIED1bhxY7PCAADAskyr5NevX6/8/HzFxcWpTZs2eu2113To0CGzwgEAeCFPPta2MjItybdt21ZvvPGG9u/fr4cfflgLFixQTEyMSkpKtGLFCuXn55sVGgDAS1i9XW/67PrAwED1799f69ev17Zt2zRixAg9//zzioiI0B133GF2eAAAVFmmJ/k/io2NVVpamvbu3av58+ebHQ4AwOKs3q6vdA/DkSRfX18lJCQoISHB7FAAABZWWdvsnlKpKnkAAOA5lbKSBwDgUrB4IU+SBwB4L9r1AACgSqKSBwB4LYsX8iR5AID3ol0PAACqJCp5AIDXsnghT5IHAHgv2vUAAKBKopIHAHgtq1fyJHkAgNeyeI6nXQ8AgFVRyQMAvBbtegAALMriOZ52PQAAVkUlDwDwWrTrAQCwKIvneNr1AABYFZU8AMBr+Vi8lCfJAwC8lsVzPO16AACsikoeAOC1mF0PAIBF+Vg7x9OuBwDAqqjkAQBei3Y9AAAWZfEcT7seAACropIHAHgtm6xdypPkAQBei9n1AACgSqKSBwB4LWbXAwBgURbP8bTrAQCwKip5AIDX4lWzAABYlMVzPO16AADM9Pzzz8tms2nYsGHOsYKCAg0aNEi1atVSUFCQkpKSlJub6/axSfIAAK9ls9k8tlyIzZs36/XXX9fVV1/tMj58+HB9/PHHWrhwodatW6d9+/YpMTHR7eOT5AEAXstm89zirmPHjun+++/XG2+8obCwMOf40aNHNXPmTL3yyivq3Lmz4uLiNGvWLH3xxRfauHGjW+cgyQMA4AEOh0N5eXkui8PhOOf2gwYNUvfu3dWlSxeX8czMTBUVFbmMN2nSRHXr1tWGDRvciokkDwDwWj42m8eW1NRUhYSEuCypqallnnfBggX6+uuvy1yfk5Mjf39/hYaGuoxHRkYqJyfHrc/H7HoAgNfy5OT6MWPGKCUlxWXMbreX2u7XX3/V0KFDtWLFClWvXt2DEZRGkgcAwAPsdnuZSf1smZmZOnDggFq1auUcKy4uVkZGhl577TUtX75chYWFOnLkiEs1n5ubq6ioKLdiIskDALyWGc+uv+mmm7Rt2zaXsX79+qlJkyYaPXq06tSpIz8/P61atUpJSUmSpKysLGVnZys+Pt6tc5HkAQBey4xXzdasWVPNmzd3GQsMDFStWrWc4wMGDFBKSorCw8MVHBysRx99VPHx8Wrbtq1b5yLJAwBQyUyYMEE+Pj5KSkqSw+FQ165dNXXqVLePQ5IHAHityvKq2bVr17r8XL16dU2ZMkVTpky5qOOWK8l/9NFH5T7gHXfcccHBAABwKVWSHF9hypXkExISynUwm82m4uLii4kHAAB4SLmSfElJSUXHAQDAJVdZ2vUVhWvyAACvZcbs+kvpgpL88ePHtW7dOmVnZ6uwsNBl3ZAhQzwSGAAAuDhuJ/ktW7botttu04kTJ3T8+HGFh4fr0KFDqlGjhiIiIkjyAIAqw+rterdfUDN8+HD16NFDv//+uwICArRx40b98ssviouL00svvVQRMQIAUCFsHlwqI7eT/NatWzVixAj5+PjI19dXDodDderUUVpamp544omKiBEAAFwAt5O8n5+ffHxO7xYREaHs7GxJUkhIiH799VfPRgcAQAXy5KtmKyO3r8m3bNlSmzdvVuPGjdWhQwc99dRTOnTokObOnVvqWbwAAFRmlTQ3e4zblfz48eMVHR0tSXruuecUFhamgQMH6uDBg5oxY4bHAwQAABfG7Uq+devWzn+OiIjQsmXLPBoQAACXitVn1/MwHACA17J4jnc/yTdo0OC8f/ns2rXrogICAACe4XaSHzZsmMvPRUVF2rJli5YtW6ZRo0Z5Ki4AACpcZZ0V7yluJ/mhQ4eWOT5lyhR99dVXFx0QAACXisVzvPuz68+lW7du+uCDDzx1OAAAcJE8NvFu0aJFCg8P99ThAACocMyuP0vLli1dvhTDMJSTk6ODBw9q6tSpHg3uQr3Z+xqzQwAqXNh1g80OAahwJ7e8VqHH91g7u5JyO8n37NnTJcn7+Piodu3a6tixo5o0aeLR4AAAwIVzO8mPHTu2AsIAAODSs3q73u1Oha+vrw4cOFBq/PDhw/L19fVIUAAAXAo+Ns8tlZHbSd4wjDLHHQ6H/P39LzogAADgGeVu10+aNEnS6dbGm2++qaCgIOe64uJiZWRkcE0eAFClVNYK3FPKneQnTJgg6XQlP336dJfWvL+/v+rXr6/p06d7PkIAACqI1a/JlzvJ7969W5LUqVMnLV68WGFhYRUWFAAAuHhuz65fs2ZNRcQBAMAlZ/V2vdsT75KSkvTCCy+UGk9LS9Pdd9/tkaAAALgUbDbPLZWR20k+IyNDt912W6nxbt26KSMjwyNBAQCAi+d2u/7YsWNl3irn5+envLw8jwQFAMClYPVXzbpdybdo0ULvvfdeqfEFCxaoWbNmHgkKAIBLwceDS2XkdiX/5JNPKjExUTt37lTnzp0lSatWrdK8efO0aNEijwcIAAAujNtJvkePHkpPT9f48eO1aNEiBQQE6JprrtHq1at51SwAoEqxeLf+wt4n3717d3Xv3l2SlJeXp/nz52vkyJHKzMxUcXGxRwMEAKCicE3+HDIyMpScnKyYmBi9/PLL6ty5szZu3OjJ2AAAwEVwq5LPycnR7NmzNXPmTOXl5alXr15yOBxKT09n0h0AoMqxeCFf/kq+R48eio2N1bfffquJEydq3759mjx5ckXGBgBAhbL6q2bLXcl/+umnGjJkiAYOHKjGjRtXZEwAAMADyl3Jr1+/Xvn5+YqLi1ObNm302muv6dChQxUZGwAAFcrHZvPYUhmVO8m3bdtWb7zxhvbv36+HH35YCxYsUExMjEpKSrRixQrl5+dXZJwAAHgcz64/S2BgoPr376/169dr27ZtGjFihJ5//nlFRETojjvuqIgYAQDABbioJ/HFxsYqLS1Ne/fu1fz58z0VEwAAlwQT78rB19dXCQkJSkhI8MThAAC4JGyqpNnZQyrrM/UBAMBF8kglDwBAVVRZ2+yeQiUPAPBaZl2TnzZtmq6++moFBwcrODhY8fHx+vTTT53rCwoKNGjQINWqVUtBQUFKSkpSbm6u+5/P7T0AAMBFueKKK/T8888rMzNTX331lTp37qyePXvq+++/lyQNHz5cH3/8sRYuXKh169Zp3759SkxMdPs8NsMwDE8Hb7aCU2ZHAFS8sOsGmx0CUOFObnmtQo//4tpdHjvWqI4NL2r/8PBwvfjii7rrrrtUu3ZtzZs3T3fddZck6aefflLTpk21YcMGtW3bttzH5Jo8AMBrefKavMPhkMPhcBmz2+2y2+3n3a+4uFgLFy7U8ePHFR8fr8zMTBUVFalLly7ObZo0aaK6deu6neRp1wMA4AGpqakKCQlxWVJTU8+5/bZt2xQUFCS73a5HHnlEH374oZo1a6acnBz5+/srNDTUZfvIyEjl5OS4FROVPADAa3nycbRjxoxRSkqKy9j5qvjY2Fht3bpVR48e1aJFi5ScnKx169Z5LiCR5AEAXsyTL5YpT2v+j/z9/dWoUSNJUlxcnDZv3qxXX31V99xzjwoLC3XkyBGXaj43N1dRUVFuxUS7HgCASqCkpEQOh0NxcXHy8/PTqlWrnOuysrKUnZ2t+Ph4t45JJQ8A8FpmPQxnzJgx6tatm+rWrav8/HzNmzdPa9eu1fLlyxUSEqIBAwYoJSVF4eHhCg4O1qOPPqr4+Hi3Jt1JJHkAgBcz6xWxBw4cUJ8+fbR//36FhITo6quv1vLly3XzzTdLkiZMmCAfHx8lJSXJ4XCoa9eumjp1qtvn4T55oIriPnl4g4q+T37y57s9dqxH2zXw2LE8hUoeAOC1fCz+FjqSPADAa5nVrr9UmF0PAIBFUckDALyW1V81S5IHAHgtTz4MpzKiXQ8AgEVRyQMAvJbFC3mSPADAe9GuBwAAVRKVPADAa1m8kCfJAwC8l9Xb2Vb/fAAAeC0qeQCA17JZvF9PkgcAeC1rp3ja9QAAWBaVPADAa1n9PnmSPADAa1k7xdOuBwDAsqjkAQBey+LdepI8AMB7Wf0WOtr1AABYFJU8AMBrWb3SJckDALwW7XoAAFAlUckDALyWtet4kjwAwIvRrgcAAFUSlTwAwGtZvdIlyQMAvBbtegAAUCVRyQMAvJa163iSPADAi1m8W0+7HgAAq6q0Sf7XX39V//79zQ4DAGBhPrJ5bKmMKm2S/+233zRnzhyzwwAAWJjN5rmlMjLtmvxHH3103vW7du26RJEAAGBNpiX5hIQE2Ww2GYZxzm2sfv8iAMBctkraZvcU09r10dHRWrx4sUpKSspcvv76a7NCAwB4Cau3601L8nFxccrMzDzn+j+r8gEAwPmZ1q4fNWqUjh8/fs71jRo10po1ay5hRAAAb1NZZ8V7imlJ/sYbbzzv+sDAQHXo0OESRQMA8EaVtc3uKZX2FjoAAHBxeKwtAMBrWb2SJ8kDALwWt9ABAIAqiSQPAPBaPjbPLe5ITU3Vddddp5o1ayoiIkIJCQnKyspy2aagoECDBg1SrVq1FBQUpKSkJOXm5rp1HlPa9X/2SNs/uuOOOyowEgCANzOrXb9u3ToNGjRI1113nU6dOqUnnnhCt9xyi3744QcFBgZKkoYPH66lS5dq4cKFCgkJ0eDBg5WYmKjPP/+83OexGSY8ccbHp3wNBJvNpuLiYrePX3DK7V2AKifsusFmhwBUuJNbXqvQ46/+6bDHjtW5Sa0L3vfgwYOKiIjQunXr1L59ex09elS1a9fWvHnzdNddd0mSfvrpJzVt2lQbNmxQ27Zty3VcUyr5kpISM04LAIALT86udzgccjgcLmN2u112u/1P9z169KgkKTw8XJKUmZmpoqIidenSxblNkyZNVLduXbeSPNfkAQBey+bB/6WmpiokJMRlSU1N/dMYSkpKNGzYMLVr107NmzeXJOXk5Mjf31+hoaEu20ZGRionJ6fcn69S3EJ3/PhxrVu3TtnZ2SosLHRZN2TIEJOiAgCg/MaMGaOUlBSXsfJU8YMGDdJ3332n9evXezwm05P8li1bdNttt+nEiRM6fvy4wsPDdejQIdWoUUMREREkeQBAhXF3Vvz5lLc1/0eDBw/WkiVLlJGRoSuuuMI5HhUVpcLCQh05csSlms/NzVVUVFS5j296u3748OHq0aOHfv/9dwUEBGjjxo365ZdfFBcXp5deesns8AAAFubJdr07DMPQ4MGD9eGHH2r16tVq0KCBy/q4uDj5+flp1apVzrGsrCxlZ2crPj6+3OcxvZLfunWrXn/9dfn4+MjX11cOh0MNGzZUWlqakpOTlZiYaHaIOIfMrzZr9lsz9eMP3+ngwYOaMGmKOt/U5c93BCqxn5Y+o3oxpWdJT38vQ8Off192/2p6PiVRd3eNk92/mlZu+FFDx7+nA7/lmxAtqqpBgwZp3rx5+ve//62aNWs6r7OHhIQoICBAISEhGjBggFJSUhQeHq7g4GA9+uijio+PL/ekO6kSJHk/Pz/nLXURERHKzs5W06ZNFRISol9//dXk6HA+J0+eUGxsrBISk5QylNu5YA03PPCifP/Qw23WKEafTH9Ui1dskSSljUxStxv+ovsfm6m8Yyc14fFeWvDyg+rcb4JZIeMimPXs+mnTpkmSOnbs6DI+a9Ys9e3bV5I0YcIE+fj4KCkpSQ6HQ127dtXUqVPdOo/pSb5ly5bavHmzGjdurA4dOuipp57SoUOHNHfuXOcsQ1RON9zYQTfcyOuAYS2Hfj/m8vPIfs21M/ugPsvcruCg6uqbEK++T8zWus0/S5IeevodffPhk/pri/r6ctseEyLGxTDryfXleURN9erVNWXKFE2ZMuWCz2P6Nfnx48crOjpakvTcc88pLCxMAwcO1MGDBzVjxgyTowPgzfyq+ar3bddpzr83SJJaNq0rf79qWr3xf48f/XlPrrL3/6Y2Vzc412EA05heybdu3dr5zxEREVq2bJlb+5f18AHD1/0ZjgBwtjs6Xa3QmgF65+NNkqSoWsFyFBbp6LGTLtsdOJynyFrBZoSIi+Rj8XfNml7JX6yyHj7w4gt//vABAPgzyQnXa/nnP2j/waNmh4IKYvPgUhmZXsk3aNBAtvP8JbVr167z7l/WwwcMX6p4ABenbnSYOreJVe+RbzjHcg7nye7vp5CgAJdqPqJWsHIP55kRJnBepif5YcOGufxcVFSkLVu2aNmyZRo1atSf7l/Wwwd4QQ2Ai/W3O+J14Ld8ffrZ986xLT9mq7DolDq1iVX6qq2SpMb1IlQ3Olybvt1tUqS4KJW1BPcQ05P80KFDyxyfMmWKvvrqq0scDdxx4vhxZWdnO3/+7969+unHHxUSEqLomBgTIwMujs1mU5+ebfXukk0qLv7fC7XyjhVodvoGvTAiUb8dPa784wV6ZfTd2vjNLmbWV1FmvWr2UjHlVbPlsWvXLl177bXKy3O/BUYlf2ls/nKTHuzXp9T4HT3v1LPjnzchIu/Cq2Yrzk1tm2jJtMFq0XOcdmQfcFl35mE4vW79/w/D+eJHDU19T7mHeRhORajoV81u2um5+RZtrgzx2LE8pdIm+bS0NE2dOlV79uxxe1+SPLwBSR7eoKKT/Je7PJfk/9qw8iV509v1LVu2dJl4ZxiGcnJydPDgQbef7AMAgDus3ayvBEm+Z8+eLknex8dHtWvXVseOHdWkSRMTIwMAoGozPcmPHTvW7BAAAN7K4qW86Q/D8fX11YEDB0qNHz58WL6+viZEBADwFma9avZSMT3Jn2ven8PhkL+//yWOBgAA6zCtXT9p0iRJp+9HffPNNxUUFORcV1xcrIyMDK7JAwAqlMUfXW9ekp8w4fS7lw3D0PTp011a8/7+/qpfv76mT59uVngAAFR5piX53btPPwKyU6dOWrx4scLCwswKBQDgpSxeyJs/u37NmjVmhwAA8FYWz/KmT7xLSkrSCy+8UGo8LS1Nd999twkRAQBgDaYn+YyMDN12222lxrt166aMjAwTIgIAeAur30Jnerv+2LFjZd4q5+fnd0EvpwEAoLysPrve9Eq+RYsWeu+990qNL1iwQM2aNTMhIgAArMH0Sv7JJ59UYmKidu7cqc6dO0uSVq1apfnz52vhwoUmRwcAsDKLF/LmJ/kePXooPT1d48eP16JFixQQEKCrr75aK1euVIcOHcwODwBgZRbP8qYneUnq3r27unfvXmr8u+++U/PmzU2ICACAqs/0a/Jny8/P14wZM/TXv/5V11xzjdnhAAAszOqz6ytNks/IyFCfPn0UHR2tl156SZ07d9bGjRvNDgsAYGE2m+eWysjUdn1OTo5mz56tmTNnKi8vT7169ZLD4VB6ejoz6wEAuEimVfI9evRQbGysvv32W02cOFH79u3T5MmTzQoHAOCFbB5cKiPTKvlPP/1UQ4YM0cCBA9W4cWOzwgAAeLPKmp09xLRKfv369crPz1dcXJzatGmj1157TYcOHTIrHAAALMe0JN+2bVu98cYb2r9/vx5++GEtWLBAMTExKikp0YoVK5Sfn29WaAAAL8Hs+goWGBio/v37a/369dq2bZtGjBih559/XhEREbrjjjvMDg8AYGFWn11vepL/o9jYWKWlpWnv3r2aP3++2eEAAFClVYon3p3N19dXCQkJSkhIMDsUAICFVdIC3GMqZZIHAOCSsHiWr1TtegAA4DlU8gAAr1VZZ8V7CkkeAOC1KuuseE+hXQ8AgEVRyQMAvJbFC3mSPADAi1k8y9OuBwDAoqjkAQBei9n1AABYFLPrAQBAlUSSBwB4LZsHF3dkZGSoR48eiomJkc1mU3p6ust6wzD01FNPKTo6WgEBAerSpYu2b9/u9ucjyQMAvJdJWf748eO65pprNGXKlDLXp6WladKkSZo+fbo2bdqkwMBAde3aVQUFBW6dh2vyAABcYt26dVO3bt3KXGcYhiZOnKh//vOf6tmzpyTp7bffVmRkpNLT09W7d+9yn4dKHgDgtWwe/J/D4VBeXp7L4nA43I5p9+7dysnJUZcuXZxjISEhatOmjTZs2ODWsUjyAACvZbN5bklNTVVISIjLkpqa6nZMOTk5kqTIyEiX8cjISOe68qJdDwCAB4wZM0YpKSkuY3a73aRoTiPJAwC8lidvk7fb7R5J6lFRUZKk3NxcRUdHO8dzc3N17bXXunUs2vUAAO9l1j1059GgQQNFRUVp1apVzrG8vDxt2rRJ8fHxbh2LSh4AgEvs2LFj2rFjh/Pn3bt3a+vWrQoPD1fdunU1bNgw/etf/1Ljxo3VoEEDPfnkk4qJiVFCQoJb5yHJAwC8llnPrv/qq6/UqVMn589nruUnJydr9uzZeuyxx3T8+HE99NBDOnLkiG644QYtW7ZM1atXd+s8NsMwDI9GXgkUnDI7AqDihV032OwQgAp3cstrFXr87N/cv8XtXOqGmzvJrixckwcAwKJo1wMAvJbFX0JHkgcAeC9eNQsAAKokKnkAgBezdilPkgcAeC3a9QAAoEqikgcAeC2LF/IkeQCA96JdDwAAqiQqeQCA1zLr2fWXCkkeAOC9rJ3jadcDAGBVVPIAAK9l8UKeJA8A8F7MrgcAAFUSlTwAwGsxux4AAKuydo6nXQ8AgFVRyQMAvJbFC3mSPADAezG7HgAAVElU8gAAr8XsegAALIp2PQAAqJJI8gAAWBTtegCA16JdDwAAqiQqeQCA12J2PQAAFkW7HgAAVElU8gAAr2XxQp4kDwDwYhbP8rTrAQCwKCp5AIDXYnY9AAAWxex6AABQJVHJAwC8lsULeZI8AMCLWTzL064HAMCiqOQBAF6L2fUAAFgUs+sBAECVZDMMwzA7CFRtDodDqampGjNmjOx2u9nhABWC33NURSR5XLS8vDyFhITo6NGjCg4ONjscoELwe46qiHY9AAAWRZIHAMCiSPIAAFgUSR4XzW636+mnn2YyEiyN33NURUy8AwDAoqjkAQCwKJI8AAAWRZIHAMCiSPI4p759+yohIcH5c8eOHTVs2LBLHsfatWtls9l05MiRS35uWB+/57AyknwV07dvX9lsNtlsNvn7+6tRo0YaN26cTp06VeHnXrx4sZ599tlybXup/4NVUFCgQYMGqVatWgoKClJSUpJyc3Mvybnhefyel23GjBnq2LGjgoOD+YMA5UKSr4JuvfVW7d+/X9u3b9eIESM0duxYvfjii2VuW1hY6LHzhoeHq2bNmh47nicNHz5cH3/8sRYuXKh169Zp3759SkxMNDssXAR+z0s7ceKEbr31Vj3xxBNmh4IqgiRfBdntdkVFRalevXoaOHCgunTpoo8++kjS/1qPzz33nGJiYhQbGytJ+vXXX9WrVy+FhoYqPDxcPXv21J49e5zHLC4uVkpKikJDQ1WrVi099thjOvvuyrPbmA6HQ6NHj1adOnVkt9vVqFEjzZw5U3v27FGnTp0kSWFhYbLZbOrbt68kqaSkRKmpqWrQoIECAgJ0zTXXaNGiRS7n+eSTT3TVVVcpICBAnTp1comzLEePHtXMmTP1yiuvqHPnzoqLi9OsWbP0xRdfaOPGjRfwDaMy4Pe8tGHDhunxxx9X27Zt3fw24a1I8hYQEBDgUsmsWrVKWVlZWrFihZYsWaKioiJ17dpVNWvW1GeffabPP/9cQUFBuvXWW537vfzyy5o9e7beeustrV+/Xr/99ps+/PDD8563T58+mj9/viZNmqQff/xRr7/+uoKCglSnTh198MEHkqSsrCzt379fr776qiQpNTVVb7/9tqZPn67vv/9ew4cP1wMPPKB169ZJOv0f6cTERPXo0UNbt27Vgw8+qMcff/y8cWRmZqqoqEhdunRxjjVp0kR169bVhg0b3P9CUSl5++85cEEMVCnJyclGz549DcMwjJKSEmPFihWG3W43Ro4c6VwfGRlpOBwO5z5z5841YmNjjZKSEueYw+EwAgICjOXLlxuGYRjR0dFGWlqac31RUZFxxRVXOM9lGIbRoUMHY+jQoYZhGEZWVpYhyVixYkWZca5Zs8aQZPz+++/OsYKCAqNGjRrGF1984bLtgAEDjHvvvdcwDMMYM2aM0axZM5f1o0ePLnWsP3r33XcNf3//UuPXXXed8dhjj5W5Dyo3fs/Pr6zzAmWpZuLfF7hAS5YsUVBQkIqKilRSUqL77rtPY8eOda5v0aKF/P39nT9/88032rFjR6nrjAUFBdq5c6eOHj2q/fv3q02bNs511apVU+vWrUu1Ms/YunWrfH191aFDh3LHvWPHDp04cUI333yzy3hhYaFatmwpSfrxxx9d4pCk+Pj4cp8D1sHvOXDxSPJVUKdOnTRt2jT5+/srJiZG1aq5/t8YGBjo8vOxY8cUFxend999t9SxateufUExBAQEuL3PsWPHJElLly7V5Zdf7rLuYp4HHhUVpcLCQh05ckShoaHO8dzcXEVFRV3wcWEufs+Bi0eSr4ICAwPVqFGjcm/fqlUrvffee4qIiFBwcHCZ20RHR2vTpk1q3769JOnUqVPKzMxUq1atyty+RYsWKikp0bp161yuhZ9xpsIqLi52jjVr1kx2u13Z2dnnrIyaNm3qnFx1xp9NnouLi5Ofn59WrVqlpKQkSaevkWZnZ1MdVWH8ngMXj4l3XuD+++/XZZddpp49e+qzzz7T7t27tXbtWg0ZMkR79+6VJA0dOlTPP/+80tPT9dNPP+kf//jHee/BrV+/vpKTk9W/f3+lp6c7j/n+++9LkurVqyebzaYlS5bo4MGDOnbsmGrWrKmRI0dq+PDhmjNnjnbu3Kmvv/5akydP1pw5cyRJjzzyiLZv365Ro0YpKytL8+bN0+zZs8/7+UJCQjRgwAClpKRozZo1yszMVL9+/RQfH88sZC9i9d9zScrJydHWrVu1Y8cOSdK2bdu0detW/fbbbxf35cG6zJ4UAPf8cUKSO+v3799v9OnTx7jssssMu91uNGzY0Pj73/9uHD161DCM0xOQhg4dagQHBxuhoaFGSkqK0adPn3NOSDIMwzh58qQxfPhwIzo62vD39zcaNWpkvPXWW87148aNM6KiogybzWYkJycbhnF6EtXEiRON2NhYw8/Pz6hdu7bRtWtXY926dc79Pv74Y6NRo0aG3W43brzxRuOtt97600lGJ0+eNP7xj38YYWFhRo0aNYw777zT2L9//3m/S1Re/J6X7emnnzYklVpmzZp1vq8TXoxXzQIAYFG06wEAsCiSPAAAFkWSBwDAokjyAABYFEkeAACLIskDAGBRJHkAACyKJA8AgEWR5IEqoG/fvkpISHD+3LFjRw0bNuySx7F27VrZbLbzPgoWQOVBkgcuQt++fWWz2WSz2eTv769GjRpp3LhxOnXqVIWed/HixXr22WfLtS2JGfBevIUOuEi33nqrZs2aJYfDoU8++USDBg2Sn5+fxowZ47JdYWGhy/vPL0Z4eLhHjgPA2qjkgYtkt9sVFRWlevXqaeDAgerSpYs++ugjZ4v9ueeeU0xMjGJjYyVJv/76q3r16qXQ0FCFh4erZ8+e2rNnj/N4xcXFSklJUWhoqGrVqqXHHntMZ79i4ux2vcPh0OjRo1WnTh3Z7XY1atRIM2fO1J49e9SpUydJUlhYmGw2m/r27StJKikpUWpqqho0aKCAgABdc801WrRokct5PvnkE1111VUKCAhQp06dXOIEUPmR5AEPCwgIUGFhoSRp1apVysrK0ooVK7RkyRIVFRWpa9euqlmzpj777DN9/vnnCgoK0q233urc5+WXX9bs2bP11ltvaf369frtt9/04Ycfnvecffr00fz58zVp0iT9+OOPev311xUUFKQ6derogw8+kCRlZWVp//79evXVVyVJqampevvttzV9+nR9//33Gj58uB544AGtW7dO0uk/RhITE9WjRw9t3bpVDz74oB5//PGK+toAVAST34IHVGl/fOVpSUmJsWLFCsNutxsjR440kpOTjcjISMPhcDi3nzt3rhEbG2uUlJQ4xxwOhxEQEGAsX77cMAzDiI6ONtLS0pzri4qKjCuuuOKcr0PNysoyJBkrVqwoM8Y1a9aUeoVpQUGBUaNGDeOLL75w2XbAgAHGvffeaxiGYYwZM8Zo1qyZy/rRo0f/6etQAVQeXJMHLtKSJUsUFBSkoqIilZSU6L777tPYsWM1aNAgtWjRwuU6/DfffKMdO3aoZs2aLscoKCjQzp07dfToUe3fv19t2rRxrqtWrZpat25dqmV/xtatW+Xr66sOHTqUO+YdO3boxIkTuvnmm13GCwsL1bJlS0nSjz/+6BKHJMXHx5f7HADMR5IHLlKnTp00bdo0+fv7KyYmRtWq/e9fq8DAQJdtjx07pri4OL377ruljlO7du0LOn9AQIDb+xw7dkyStHTpUl1++eUu6+x2+wXFAaDyIckDFykwMFCNGjUq17atWrXSe++9p4iICAUHB5e5TXR0tDZt2qT27dtLkk6dOqXMzEy1atWqzO1btGihkpISrVu3Tl26dCm1/kwnobi42DnWrFkz2e12ZWdnn7MD0LRpU3300UcuYxs3bvzzDwmg0mDiHXAJ3X///brsssvUs2dPffbZZ9q9e7fWrl2rIUOGaO/evZKkoUOH6vnnn1d6erp++ukn/eMf/zjvPe7169dXcnKy+vfvr/T0dOcx33//fUlSvXr1ZLPZtGTJEh08eFDHjh1TzZo1NXLkSA0fPlxz5szRzp079fXXX2vy5MmaM2eOJOmRRx7R9u3bNWrUKGVlZWnevHmaPXt2RX9FADyIJA9cQjVq1FBGRobq1q2rxMRENW3aVAMGDFBBQYGzsh8xYoT+9re/KTk5WfHx8apZs6buvPPO8x532rRpuuuuu/SPf/xDTZo00d///ncdP35cknT55ZfrmWee0eOPP67IyEgNHjxYkvTss8/qySefVGpqqpo2bapbb71VS5cuVYMGDSRJdevW1QcffKD09HRdc801mj59usaPH1+B3w4AT7MZ55rNAwAAqjQqeQAALIokDwCARZHkAQCwKJI8AAAWRZIHAMCiSPIAAFgUSR4AAIsiyQMAYFEkeQAALIokDwCARZHkAQCwqP8HRwhQjv7U4nAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1zjuz0ss0Hg",
        "outputId": "078c78ea-0964-4dd2-ba81-694a988e138b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.95\n",
            "Recall:    0.99\n",
            "F1-Score:  0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1 → imbalanced\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 3: Train Logistic Regression without class weights\n",
        "model_no_weights = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with class weights balanced\n",
        "model_weighted = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate and compare\n",
        "print(\"Without Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"With Class Weights (balanced):\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "print(f\"Accuracy without weights: {accuracy_score(y_test, y_pred_no_weights):.2f}\")\n",
        "print(f\"Accuracy with weights:    {accuracy_score(y_test, y_pred_weighted):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AEsm1XQs0p5",
        "outputId": "81dfa2bd-5fd5-42cd-919a-383792043ac2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Class Weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       180\n",
            "           1       0.94      0.85      0.89        20\n",
            "\n",
            "    accuracy                           0.98       200\n",
            "   macro avg       0.96      0.92      0.94       200\n",
            "weighted avg       0.98      0.98      0.98       200\n",
            "\n",
            "With Class Weights (balanced):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96       180\n",
            "           1       0.58      0.90      0.71        20\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.78      0.91      0.83       200\n",
            "weighted avg       0.95      0.93      0.93       200\n",
            "\n",
            "Accuracy without weights: 0.98\n",
            "Accuracy with weights:    0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Select relevant features and target\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Step 4: Encode categorical variables\n",
        "df['sex'] = LabelEncoder().fit_transform(df['sex'])\n",
        "df['embarked'] = LabelEncoder().fit_transform(df['embarked'])\n",
        "\n",
        "# Step 5: Split data\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:   \", recall_score(y_test, y_pred))\n",
        "print(\"F1-Score: \", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnnClZETs0vJ",
        "outputId": "0b916ca9-6e90-490e-80cb-180ed58fe91e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8100558659217877\n",
            "Precision: 0.7857142857142857\n",
            "Recall:    0.7432432432432432\n",
            "F1-Score:  0.7638888888888888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-9ec60ffa0c86>:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['age'].fillna(df['age'].median(), inplace=True)\n",
            "<ipython-input-16-9ec60ffa0c86>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Step 4: Apply Standardization (Z-score normalization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train with scaled data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print comparison\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggHETNM5s0zf",
        "outputId": "1878bc94-4774-4a98-addd-b333fd90fa1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.96\n",
            "Accuracy with scaling:    0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Step 5: Calculate ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Step 6: Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "_GdVLqREs03v",
        "outputId": "3a38325e-46b6-47bb-8e46-7b70934b7780"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with C=0.5 (custom learning rate control)\n",
        "model = LogisticRegression(C=0.5, solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71uoYzgPu8n1",
        "outputId": "6175e9b2-671f-4b4e-c85c-3682a570f32b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Abs_Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Step 5: Sort features by importance\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Step 6: Display top features\n",
        "print(\"Top 10 important features based on Logistic Regression coefficients:\\n\")\n",
        "print(feature_importance.head(10)[['Feature', 'Coefficient']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2QqREYvCb_",
        "outputId": "54ae15b5-f256-46fa-a9e8-2ab13e0d60d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 important features based on Logistic Regression coefficients:\n",
            "\n",
            "                 Feature  Coefficient\n",
            "0            mean radius     2.099812\n",
            "26       worst concavity    -1.571347\n",
            "11         texture error     1.122469\n",
            "25     worst compactness    -1.004435\n",
            "20          worst radius     0.966303\n",
            "28        worst symmetry    -0.840956\n",
            "27  worst concave points    -0.693514\n",
            "6         mean concavity    -0.691207\n",
            "7    mean concave points    -0.408107\n",
            "5       mean compactness    -0.379844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOm8Sq0tvCIe",
        "outputId": "38bbb177-7882-4cdf-bf60-357935b2491d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute Precision-Recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 6: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"AP = {avg_precision:.2f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "dYo36QNavB1d",
        "outputId": "1d5bf4f3-e5e7-4def-b0b4-718f03d687ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Step 4: Train and evaluate models for each solver\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[solver] = acc\n",
        "    except Exception as e:\n",
        "        results[solver] = f\"Error: {str(e)}\"\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"Accuracy comparison of solvers:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gg37KzZvBm5",
        "outputId": "d21783ec-7aa1-4e77-d1ce-16c9ea04aba0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison of solvers:\n",
            "liblinear: 0.956140350877193\n",
            "saga: 0.9649122807017544\n",
            "lbfgs: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 6: Output result\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8HdvBv2wZZR",
        "outputId": "009e1752-d59b-4f54-8289-4d62b9ccf3c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Compare results\n",
        "print(f\"Accuracy without scaling:  {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with scaling:     {acc_scaled:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ayO2-2Rwcea",
        "outputId": "59c27c04-f0ff-4ad8-8ea9-f3e68a4e2e7a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling:  0.9561\n",
            "Accuracy with scaling:     0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Set up Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear')  # liblinear supports L1/L2\n",
        "\n",
        "# Step 4: Define grid of C values to search\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 50]}\n",
        "\n",
        "# Step 5: Setup GridSearchCV with 5-fold CV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Step 6: Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Print best C and corresponding accuracy on test set\n",
        "print(f\"Best C: {grid_search.best_params_['C']}\")\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(f\"Test Accuracy with best C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSqkVsdIwb-Y",
        "outputId": "711d1ea3-9202-4790-a640-06448030ed39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 10\n",
            "Test Accuracy with best C: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model to a file\n",
        "filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, filename)\n",
        "\n",
        "# Step 5: Load the model from file\n",
        "loaded_model = joblib.load(filename)\n",
        "\n",
        "# Step 6: Use loaded model to make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 7: Print some predictions\n",
        "print(\"Predictions from loaded model:\", y_pred[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFP21lUwwbyL",
        "outputId": "61639938-45e3-4a4d-f513-b4a4aa89551f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions from loaded model: [1 0 0 1 1 0 0 0 1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}